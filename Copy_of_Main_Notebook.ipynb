{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2by81Ycp1Ke"
      },
      "source": [
        "# Cart2Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4dO5a_ouRjy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def minimum_bounding_rectangle(points):\n",
        "    \"\"\"\n",
        "    Find the smallest bounding rectangle for a set of points.\n",
        "    Returns a set of points representing the corners of the bounding box.\n",
        "\n",
        "    :param points: an nx2 matrix of coordinates\n",
        "    :rval: an nx2 matrix of coordinates\n",
        "    \"\"\"\n",
        "    from scipy.ndimage.interpolation import rotate\n",
        "    pi2 = np.pi/2.\n",
        "\n",
        "    # get the convex hull for the points\n",
        "    hull_points = points[ConvexHull(points).vertices]\n",
        "\n",
        "    # calculate edge angles\n",
        "    edges = np.zeros((len(hull_points)-1, 2))\n",
        "    edges = hull_points[1:] - hull_points[:-1]\n",
        "\n",
        "    angles = np.zeros((len(edges)))\n",
        "    angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
        "\n",
        "    angles = np.abs(np.mod(angles, pi2))\n",
        "    angles = np.unique(angles)\n",
        "\n",
        "    # find rotation matrices\n",
        "    # XXX both work\n",
        "    rotations = np.vstack([\n",
        "        np.cos(angles),\n",
        "        np.cos(angles-pi2),\n",
        "        np.cos(angles+pi2),\n",
        "        np.cos(angles)]).T\n",
        "    rotations = rotations.reshape((-1, 2, 2))\n",
        "\n",
        "    # apply rotations to the hull\n",
        "    rot_points = np.dot(rotations, hull_points.T)\n",
        "\n",
        "    #bounding points\n",
        "    min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
        "    max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
        "    min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
        "    max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
        "\n",
        "    #box with best area\n",
        "    areas = (max_x - min_x) * (max_y - min_y)\n",
        "    best_idx = np.argmin(areas)\n",
        "\n",
        "    #best box\n",
        "    x1 = max_x[best_idx]\n",
        "    x2 = min_x[best_idx]\n",
        "    y1 = max_y[best_idx]\n",
        "    y2 = min_y[best_idx]\n",
        "    r = rotations[best_idx]\n",
        "\n",
        "    rval = np.zeros((4, 2))\n",
        "    rval[0] = np.dot([x1, y2], r)\n",
        "    rval[1] = np.dot([x2, y2], r)\n",
        "    rval[2] = np.dot([x2, y1], r)\n",
        "    rval[3] = np.dot([x1, y1], r)\n",
        "\n",
        "    return rval\n",
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "def ConvPixel(FVec, xp, yp, A, B, base=1, custom_cut=None, index=0):\n",
        "    n = len(FVec)\n",
        "    M = np.ones([int(A), int(B)]) * base\n",
        "    for j in range(0, n):\n",
        "        # M[int(xp[j]) - 1, int(yp[j]) - 1] = 0\n",
        "        M[int(xp[j]) - 1, int(yp[j]) - 1] = FVec[j]\n",
        "    zp = np.array([xp, yp])\n",
        "    dup = {}\n",
        "    # find duplicate\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "                \n",
        "    for index in dup.keys():\n",
        "        x, y = index.split(\"-\")\n",
        "        M[int(float(x)) - 1, int(float(y)) - 1] = sum(FVec[list(dup[index])]) / len(dup[index])\n",
        "    if custom_cut is not None:\n",
        "      M = np.delete(M, range(0, custom_cut), 0)\n",
        "    return M\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import json as json\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_duplicate(zp):\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "    sum = 0\n",
        "    for ind in dup.keys():\n",
        "        sum = sum + (len(dup[ind]) - 1)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def dataset_with_best_duplicates(X, y, zp):\n",
        "    X = X.transpose()\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "    toDelete = []\n",
        "    for index in dup.keys():\n",
        "        mi = []\n",
        "        x_new = X[:, list(dup[index])]\n",
        "        mi = mutual_info_classif(x_new, y)\n",
        "        max = np.argmax(mi)\n",
        "        dup[index].remove(list(dup[index])[max])\n",
        "        toDelete.extend(list(dup[index]))\n",
        "    X = np.delete(X, toDelete, axis=1)\n",
        "    zp = np.delete(zp, toDelete, axis=1)\n",
        "    return X.transpose(), zp, toDelete\n",
        "\n",
        "def count_model_col(rotatedData,Q,r1,r2):\n",
        "    tot = []\n",
        "    for f in range(r1-1, r2):\n",
        "        A = f\n",
        "        B = f\n",
        "        xp = np.round(\n",
        "            1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "        yp = np.round(\n",
        "            1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "        zp = np.array([xp, yp])\n",
        "        A = max(xp)\n",
        "        B = max(yp)\n",
        "\n",
        "        # find duplicates\n",
        "        sum=str(find_duplicate(zp))\n",
        "        print(\"Collisioni: \" + sum)\n",
        "        tot.append([A,sum])\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.savefig(str(A)+'.png')\n",
        "        plt.show()\n",
        "    pd.DataFrame(tot).to_csv(\"Collision_autoencoder.csv\")\n",
        "\n",
        "\n",
        "def Cart2Pixel(Q=None, A=None, B=None, dynamic_size=False, mutual_info=False, only_model=False, params=None):\n",
        "    # TODO controls on input\n",
        "    if A is not None:\n",
        "        A = A - 1\n",
        "    if (B != None):\n",
        "        B = B - 1\n",
        "    # to dataframe\n",
        "    feat_cols = [\"col-\" + str(i + 1) for i in range(Q[\"data\"].shape[1])]\n",
        "    df = pd.DataFrame(Q[\"data\"], columns=feat_cols)\n",
        "    if Q[\"method\"] == 'pca':\n",
        "        pca = PCA(n_components=2)\n",
        "        Y = pca.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'tSNE':\n",
        "        tsne = TSNE(n_components=2, method=\"exact\")\n",
        "        Y = tsne.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'kpca':\n",
        "        kpca = KernelPCA(n_components=2, kernel='linear')\n",
        "        Y = kpca.fit_transform(df)\n",
        "\n",
        "    x = Y[:, 0]\n",
        "    y = Y[:, 1]\n",
        "    n, n_sample = Q[\"data\"].shape\n",
        "    bbox = minimum_bounding_rectangle(Y)\n",
        "    grad = (bbox[1, 1] - bbox[0, 1]) / (bbox[1, 0] - bbox[0, 0])\n",
        "    theta = np.arctan(grad)\n",
        "    R = np.asmatrix([[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]])\n",
        "    bboxMatrix = np.matrix(bbox)\n",
        "    zrect = (R.dot(bboxMatrix.transpose())).transpose()\n",
        "    coord = np.array([x, y])\n",
        "    rotatedData = np.array(R.dot(coord))  # Z\n",
        "    plt.scatter(rotatedData[0, :], rotatedData[1:])\n",
        "    plt.axis('square')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    # find duplicate\n",
        "    for i in range(len(rotatedData[0, :])):\n",
        "        for j in range(i + 1, len(rotatedData[0])):\n",
        "            if rotatedData[0, i] == rotatedData[0, j] and rotatedData[1, i] == rotatedData[1, j]:\n",
        "                print(\"duplicate:\" + str(i) + \" \" + str(j))\n",
        "\n",
        "    # nearest point\n",
        "\n",
        "    min_dist = np.inf\n",
        "    min_p1 = 0\n",
        "    min_p2 = 0\n",
        "    for p1 in range(n):\n",
        "        for p2 in range(p1 + 1, n):\n",
        "            d = (rotatedData[0, p1] - rotatedData[0, p2]) ** 2 + (rotatedData[1, p1] - rotatedData[1, p2]) ** 2\n",
        "            if min_dist > d > 0 and p1 != p2:\n",
        "                min_p1 = p1\n",
        "                min_p2 = p2\n",
        "                min_dist = d\n",
        "\n",
        "    # euclidean distance\n",
        "    dmin = np.linalg.norm(rotatedData[:, min_p1] - rotatedData[:, min_p2])\n",
        "    rec_x_axis = abs(zrect[0, 0] - zrect[1, 0])\n",
        "    rec_y_axis = abs(zrect[1, 1] - zrect[2, 1])\n",
        "\n",
        "    if dynamic_size:\n",
        "        precision_old = math.sqrt(2)\n",
        "        A = math.ceil(rec_x_axis * precision_old / dmin)\n",
        "        B = math.ceil(rec_y_axis * precision_old / dmin)\n",
        "        print(\"Dynamic [A:\" + str(A) + \" ; B:\" + str(B) + \"]\")\n",
        "        if A > Q[\"max_A_size\"] or B > Q[\"max_B_size\"]:\n",
        "            precision = precision_old * (Q[\"max_A_size\"] / A) * (Q[\"max_B_size\"] / B)\n",
        "            A = math.ceil(rec_x_axis * precision / dmin)\n",
        "            B = math.ceil(rec_y_axis * precision / dmin)\n",
        "    # cartesian coordinates to pixels\n",
        "    tot = []\n",
        "    xp = np.round(\n",
        "        1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "    yp = np.round(\n",
        "        1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "    # Modified Feature Position | custom cut\n",
        "    cut = params[\"cut\"]\n",
        "    if cut is not None:\n",
        "      assert True\n",
        "      xp[59] = cut\n",
        "\n",
        "    zp = np.array([xp, yp])\n",
        "    A = max(xp)\n",
        "    B = max(yp)\n",
        "\n",
        "    # find duplicates\n",
        "    print(\"Collisioni: \" + str(find_duplicate(zp)))\n",
        "\n",
        "\n",
        "    # Training set\n",
        "\n",
        "    images = []\n",
        "    toDelete = 0\n",
        "    name = \"_\" + str(int(A)) + 'x' + str(int(B))\n",
        "    if params[\"No_0_MI\"]:\n",
        "        name = name + \"_No_0_MI\"\n",
        "    if mutual_info:\n",
        "        print(\"calc MI\")\n",
        "        Q[\"data\"], zp, toDelete = dataset_with_best_duplicates(Q[\"data\"], Q[\"y\"], zp)\n",
        "        name = name + \"_MI\"\n",
        "        print(\"MI done\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        name = name + \"_Mean\"\n",
        "    if cut is not None:\n",
        "        name = name + \"_Cut\"+str(cut)\n",
        "    if only_model:\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        a=ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B, index=i)\n",
        "        plt.imshow(a,cmap=\"gray\")\n",
        "        plt.show()\n",
        "        print(\"Create images\")\n",
        "        if cut is not None:\n",
        "          images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, custom_cut=cut-1, index=i) for i in range(0, n_sample)]\n",
        "        else:\n",
        "          images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, index=i) for i in range(0, n_sample)]\n",
        "\n",
        "        filename = params[\"dir\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = params[\"res\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "    image_model = {\"xp\": zp[0].tolist(), \"yp\": zp[1].tolist(), \"A\": A, \"B\": B, \"custom_cut\": cut}\n",
        "\n",
        "\n",
        "    return images, image_model, toDelete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3-ITpNnp9vR"
      },
      "source": [
        "# Neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTVqqtzop6T5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Activation, AveragePooling2D, Add, \\\n",
        "    Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "\n",
        "def CNN_Nature(images, y, param=None):\n",
        "    print(param)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100)\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "    num_filters = param[\"filter\"]\n",
        "    num_filters2 = param[\"filter2\"]\n",
        "\n",
        "    kernel = param[\"kernel\"]\n",
        "\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "    print(x_train.shape)\n",
        "    out = Conv2D(filters=num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(inputs)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=2 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=4 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "\n",
        "    # layer 2\n",
        "    out2 = Conv2D(filters=num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(inputs)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=2 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=4 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "\n",
        "    # final layer\n",
        "    outf = Concatenate()([out, out2])\n",
        "    out_f = AveragePooling2D(strides=2, pool_size=2)(outf)\n",
        "    out_f = Flatten()(out_f)\n",
        "    predictions = Dense(5, activation='softmax')(out_f)\n",
        "\n",
        "    # This creates a model that includes\n",
        "    # the Input layer and three Dense layers\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    adam = Adam(lr=param[\"learning_rate\"])\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=adam,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    y_train_labels = np.argmax(y_train, axis=1)\n",
        "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_labels), y=y_train_labels)\n",
        "    class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}  # Convert to dictionary format\n",
        "\n",
        "    # Train the model.\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=param[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=param[\"batch\"],\n",
        "        class_weight=class_weights_dict,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.keras')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n",
        "\n",
        "\n",
        "def CNN2(images, y, params=None):\n",
        "    print(params)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100\n",
        "                                                        )\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "\n",
        "    kernel = params[\"kernel\"]\n",
        "    kernel2=int(kernel/2)\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "\n",
        "    X = Conv2D(32, (kernel,kernel), activation='relu', name='conv0')(inputs)\n",
        "    X = Dropout(rate=params['dropout1'])(X)\n",
        "    X = Conv2D(64, (kernel, kernel), activation='relu', name='conv1')(X)\n",
        "    X = Dropout(rate=params['dropout2'])(X)\n",
        "    X = Conv2D(128, (kernel, kernel), activation='relu', name='conv2')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(1024, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(5, activation='softmax', kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "    model = Model(inputs, X)\n",
        "    adam = Adam(params[\"learning_rate\"])\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    y_train_labels = np.argmax(y_train, axis=1)\n",
        "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_labels), y=y_train_labels)\n",
        "    class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}  # Convert to dictionary format\n",
        "\n",
        "    # Train the model.\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=params[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=params[\"batch\"],\n",
        "        class_weight=class_weights_dict,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.keras')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCBvyo-1uWzy"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFBUf1WUuaNL"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from hyperopt import STATUS_OK\n",
        "from hyperopt import tpe, hp, Trials, fmin\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "\n",
        "import time\n",
        "\n",
        "XGlobal = []\n",
        "YGlobal = []\n",
        "\n",
        "XTestGlobal = []\n",
        "YTestGlobal = []\n",
        "\n",
        "SavedParameters = []\n",
        "Mode = \"\"\n",
        "Name = \"\"\n",
        "best_val_acc = 0\n",
        "path_model=\"\"\n",
        "\n",
        "\n",
        "ds = 1\n",
        "train = 0\n",
        "test = 0\n",
        "\n",
        "if ds == 1:\n",
        "    #train = 20000/100*20  # attacchi\n",
        "    train=6857\n",
        "    test = 180000\n",
        "elif ds == 2:\n",
        "    train = 9067\n",
        "    test = 119341\n",
        "else:\n",
        "    train = 79349\n",
        "    test = 250436\n",
        "\n",
        "def fix(f):\n",
        "    a = f[\"TN_val\"]\n",
        "    b = f[\"FP_val\"]\n",
        "    c = f[\"FN_val\"]\n",
        "    d = f[\"TP_val\"]\n",
        "    f[\"TN_val\"] = d\n",
        "    f[\"TP_val\"] = a\n",
        "    f[\"FP_val\"] = c\n",
        "    f[\"FN_val\"] = b\n",
        "    return f\n",
        "\n",
        "\n",
        "def fix_test(f):\n",
        "    a = f[\"TN_test\"]\n",
        "    b = f[\"FP_test\"]\n",
        "    c = f[\"FN_test\"]\n",
        "    d = f[\"TP_test\"]\n",
        "    f[\"TN_test\"] = d\n",
        "    f[\"TP_test\"] = a\n",
        "    f[\"FP_test\"] = c\n",
        "    f[\"FN_test\"] = b\n",
        "    return f\n",
        "\n",
        "def res(cm, val):\n",
        "    tp = cm[0][0]  # attacks true\n",
        "    fn = cm[0][1]  # attacs predict normal\n",
        "    fp = cm[1][0]  # normal predict attacks\n",
        "    tn = cm[1][1]  # normal as normal\n",
        "    attacks = tp + fn\n",
        "    normals = fp + tn\n",
        "    print(attacks)\n",
        "    print(normals)\n",
        "\n",
        "\n",
        "    if attacks <= normals:\n",
        "        print(\"ok\")\n",
        "    elif not val:\n",
        "        print(\"error\")\n",
        "        return False,False\n",
        "    OA = (tp + tn) / (attacks + normals)\n",
        "    AA = ((tp / attacks) + (tn / normals)) / 2\n",
        "    P = tp / (tp + fp)\n",
        "    R = tp / (tp + fn)\n",
        "    F1 = 2 * ((P * R) / (P + R))\n",
        "    FAR = fp / (fp + tn)\n",
        "    TPR = tp / (tp + fn)\n",
        "    r = [OA, AA, P, R, F1, FAR, TPR]\n",
        "    return True,r\n",
        "\n",
        "def hyperopt_fcn(params):\n",
        "    if Mode == \"CNN_Nature\":\n",
        "      if params[\"filter\"] == params[\"filter2\"] :\n",
        "          return {'loss': np.inf, 'status': STATUS_OK}\n",
        "    global SavedParameters\n",
        "    start_time = time.time()\n",
        "    print(\"start train\")\n",
        "    if Mode == \"CNN_Nature\":\n",
        "        model, val = CNN_Nature(XGlobal, YGlobal, params)\n",
        "    elif Mode == \"CNN2\":\n",
        "        model, val = CNN2(XGlobal, YGlobal, params)\n",
        "    print(\"start predict\")\n",
        "    print(XTestGlobal.shape)\n",
        "    print(YTestGlobal.shape)\n",
        "    Y_predicted = model.predict(XTestGlobal, verbose=0)\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    cf = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "    K.clear_session()\n",
        "    SavedParameters.append(val)\n",
        "    global best_val_acc\n",
        "\n",
        "    if Mode == \"CNN_Nature\":\n",
        "        SavedParameters[-1].update({\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) *\n",
        "                                                              100, \"TP_test\": cf[0][0], \"FN_test\": cf[0][1],\n",
        "                                    \"FP_test\": cf[1][0], \"TN_test\": cf[1][1], \"kernel\": params[\n",
        "                \"kernel\"], \"learning_rate\": params[\"learning_rate\"], \"batch\": params[\"batch\"],\n",
        "                                    \"filter1\": params[\"filter\"],\n",
        "                                    \"filter2\": params[\"filter2\"],\n",
        "                                    \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    elif Mode == \"CNN2\":\n",
        "        SavedParameters[-1].update(\n",
        "            {\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) * 100, \"TN_test\": cf[0][0],\n",
        "             \"FP_test\": cf[0][1], \"FN_test\": cf[1][0], \"TP_test\": cf[1][1], \"kernel\": params[\"kernel\"],\n",
        "             \"learning_rate\": params[\"learning_rate\"],\n",
        "             \"batch\": params[\"batch\"],\n",
        "             \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "              [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "\n",
        "    done,r = res(cm_val, True)\n",
        "    if not done:\n",
        "        SavedParameters[-1]=fix(SavedParameters[-1])\n",
        "        cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "              [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "        done, r = res(cm_val, True)\n",
        "    assert done==True\n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_val\": r[0],\n",
        "        \"P_val\": r[2],\n",
        "        \"R_val\": r[3],\n",
        "        \"F1_val\": r[4],\n",
        "        \"FAR_val\": r[5],\n",
        "        \"TPR_val\": r[6]\n",
        "    })\n",
        "    cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]],\n",
        "               [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "    done, r = res(cm_test, False)\n",
        "    if not done:\n",
        "        SavedParameters[-1]=fix_test(SavedParameters[-1])\n",
        "        cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]], [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "        done, r = res(cm_test, False)\n",
        "    assert done==True\n",
        "\n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_test\": r[0],\n",
        "        \"P_test\": r[2],\n",
        "        \"R_test\": r[3],\n",
        "        \"F1_test\": r[4],\n",
        "        \"FAR_test\": r[5],\n",
        "        \"TPR_test\": r[6]\n",
        "    })\n",
        "    #Save Model\n",
        "    if SavedParameters[-1][\"F1_val\"] > best_val_acc:\n",
        "        print(\"new saved model:\" + str(SavedParameters[-1]))\n",
        "        model.save(path_model)\n",
        "        best_val_acc = SavedParameters[-1][\"F1_val\"]\n",
        "\n",
        "\n",
        "    SavedParameters = sorted(SavedParameters, key=lambda i: i['F1_test'], reverse=True)\n",
        "\n",
        "    try:\n",
        "        with open(Name, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=SavedParameters[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(SavedParameters)\n",
        "    except IOError:\n",
        "        print(\"I/O error\")\n",
        "    print(\"prova\"+str(val[\"F1_val\"]))\n",
        "    return {'loss': -val[\"F1_test\"], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def train_norm(param, dataset, norm):\n",
        "    np.random.seed(param[\"seed\"])\n",
        "    print(\"modelling dataset\")\n",
        "    global YGlobal\n",
        "    YGlobal = to_categorical(dataset[\"Classification\"])\n",
        "    del dataset[\"Classification\"]\n",
        "    global YTestGlobal\n",
        "    YTestGlobal = to_categorical(dataset[\"Ytest\"])\n",
        "    del dataset[\"Ytest\"]\n",
        "\n",
        "    global XGlobal\n",
        "    global XTestGlobal\n",
        "\n",
        "    if not param[\"LoadFromJson\"]:\n",
        "        # norm\n",
        "        Out = {}\n",
        "        if norm:\n",
        "            print('NORM Min-Max')\n",
        "            Out[\"Max\"] = float(dataset[\"Xtrain\"].max().max())\n",
        "            Out[\"Min\"] = float(dataset[\"Xtrain\"].min().min())\n",
        "            # NORM\n",
        "            dataset[\"Xtrain\"] = (dataset[\"Xtrain\"] - Out[\"Min\"]) / (Out[\"Max\"] - Out[\"Min\"])\n",
        "            dataset[\"Xtrain\"] = dataset[\"Xtrain\"].fillna(0)\n",
        "\n",
        "        # TODO implement norm 2\n",
        "        print(\"trasposing\")\n",
        "\n",
        "        q = {\"data\": np.array(dataset[\"Xtrain\"].values).transpose(), \"method\": param[\"Metod\"],\n",
        "             \"max_A_size\": param[\"Max_A_Size\"], \"max_B_size\": param[\"Max_B_Size\"], \"y\": np.argmax(YGlobal, axis=1)}\n",
        "        print(q[\"method\"])\n",
        "        print(q[\"max_A_size\"])\n",
        "        print(q[\"max_B_size\"])\n",
        "\n",
        "        # generate images\n",
        "        XGlobal, image_model, toDelete = Cart2Pixel(q, q[\"max_A_size\"], q[\"max_B_size\"], param[\"Dynamic_Size\"],\n",
        "                                                    mutual_info=param[\"mutual_info\"], params=param)\n",
        "\n",
        "        del q[\"data\"]\n",
        "        print(\"Train Images done!\")\n",
        "        # generate testingset image\n",
        "        if param[\"mutual_info\"]:\n",
        "            dataset[\"Xtest\"] = dataset[\"Xtest\"].drop(dataset[\"Xtest\"].columns[toDelete], axis=1)\n",
        "\n",
        "        dataset[\"Xtest\"] = np.array(dataset[\"Xtest\"]).transpose()\n",
        "        print(\"generating Test Images\")\n",
        "        print(dataset[\"Xtest\"].shape)\n",
        "        if image_model[\"custom_cut\"] is not None:\n",
        "          XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                  image_model[\"A\"], image_model[\"B\"],custom_cut=image_model[\"custom_cut\"])\n",
        "                        for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        else:\n",
        "          XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                  image_model[\"A\"], image_model[\"B\"])\n",
        "                        for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        print(\"Test Images done!\")\n",
        "\n",
        "        # saving testingset\n",
        "        name = \"_\" + str(int(q[\"max_A_size\"])) + \"x\" + str(int(q[\"max_B_size\"]))\n",
        "        if param[\"No_0_MI\"]:\n",
        "            name = name + \"_No_0_MI\"\n",
        "        if param[\"mutual_info\"]:\n",
        "            name = name + \"_MI\"\n",
        "        else:\n",
        "            name = name + \"_Mean\"\n",
        "        if image_model[\"custom_cut\"] is not None:\n",
        "            name = name + \"_Cut\" + str(image_model[\"custom_cut\"])\n",
        "        filename = param[\"dir\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "        filename = param[\"res\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "    else:\n",
        "        XGlobal = dataset[\"Xtrain\"]\n",
        "        XTestGlobal = dataset[\"Xtest\"]\n",
        "    del dataset[\"Xtrain\"]\n",
        "    del dataset[\"Xtest\"]\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    image_size1 = XTestGlobal.shape[1]\n",
        "    image_size2 = XTestGlobal.shape[2]\n",
        "    print(\"shape\" + str(XTestGlobal.shape))\n",
        "    XTestGlobal = np.reshape(XTestGlobal, [-1, image_size1, image_size2, 1])\n",
        "    YTestGlobal = np.argmax(YTestGlobal, axis=1)\n",
        "    print(XTestGlobal.shape)\n",
        "    print(YTestGlobal.shape)\n",
        "  \n",
        "    if param[\"Mode\"] == \"CNN_Nature\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 4 + 1)),\n",
        "                                \"filter\": hp.choice(\"filter\", [16, 32, 64, 128]),\n",
        "                                \"filter2\": hp.choice(\"filter2\", [16, 32, 64, 128]),\n",
        "                                \"batch\": hp.choice(\"batch\", [16]),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\", 0.0001, 0.01),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    elif param[\"Mode\"] == \"CNN2\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 4 + 1)),\n",
        "                                \"batch\": hp.choice(\"batch\", [32, 64, 128, 256, 512]),\n",
        "                                'dropout1': hp.uniform(\"dropout1\", 0, 1),\n",
        "                                'dropout2': hp.uniform(\"dropout2\", 0, 1),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\",  0.0001, 0.001),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    global Mode\n",
        "    Mode = param[\"Mode\"]\n",
        "\n",
        "    global Name\n",
        "    Name = param[\"res\"] + \"res_\" + str(int(param[\"Max_A_Size\"])) + \"x\" + str(int(param[\"Max_B_Size\"]))\n",
        "    if param[\"No_0_MI\"]:\n",
        "        Name = Name + \"_No_0_MI\"\n",
        "    if param[\"mutual_info\"]:\n",
        "        Name = Name + \"_MI\"\n",
        "    else:\n",
        "        Name = Name + \"_Mean\"\n",
        "    Name = Name + \"_\" + Mode + \".csv\"\n",
        "\n",
        "    global path_model\n",
        "    path_model=Name.replace(\".csv\",\"_model.h5\")\n",
        "\n",
        "    trials = Trials()\n",
        "    fmin(hyperopt_fcn, optimizable_variable, trials=trials, algo=tpe.suggest, max_evals=param[\"hyper_opt_evals\"])\n",
        "\n",
        "    print(\"done\")\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddltk-WykX1z"
      },
      "source": [
        "# ACGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sE-vBLJ4kdFi",
        "outputId": "c6f86ec4-9642-4882-ed8e-65da7260497d"
      },
      "outputs": [],
      "source": [
        "# --- MULTI CLASS CLASSIFICATION ---\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Configure TensorFlow session\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True  # Dynamically grow memory used on GPU\n",
        "session = tf.compat.v1.InteractiveSession(config=config)\n",
        "\n",
        "# Load trained ACGAN model\n",
        "pathModel = 'D:/IDS/Code/Datasets/NSLKDD/acgan_aagm.h5'\n",
        "generator = load_model(pathModel, compile=False)  # Load without compiling\n",
        "\n",
        "# Parameters\n",
        "param = {\n",
        "    \"Max_A_Size\": 10, \"Max_B_Size\": 10, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180,\n",
        "    \"dir\": \"D:/IDS/Code/Datasets/NSLKDD/\", \"Mode\": \"CNN2\",  # Mode: CNN_Nature, CNN2\n",
        "    \"LoadFromJson\": False, \"mutual_info\": True,  # Mean or MI\n",
        "    \"hyper_opt_evals\": 20, \"epoch\": 150, \"No_0_MI\": False,  # True -> Removing 0 MI Features\n",
        "    \"autoencoder\": False, \"cut\": None\n",
        "}\n",
        "\n",
        "dim = 53330  # Number of synthetic samples to generate\n",
        "\n",
        "# Load dataset\n",
        "if param['mutual_info']:\n",
        "    method = 'MI'\n",
        "else:\n",
        "    method = 'Mean'\n",
        "\n",
        "with open(param[\"dir\"] + f'train_{param[\"Max_A_Size\"]}x{param[\"Max_B_Size\"]}_{method}.pickle', 'rb') as f:\n",
        "    Xtrain = pickle.load(f)\n",
        "\n",
        "with open(param[\"dir\"] + 'YTrain.pickle', 'rb') as f:\n",
        "    Ytrain = pickle.load(f)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "Xtrain = np.array(Xtrain)\n",
        "Ytrain = np.array(Ytrain)\n",
        "\n",
        "print(\"Original Training Data Shape:\", Xtrain.shape)\n",
        "print(\"Original Labels Shape:\", Ytrain.shape)\n",
        "\n",
        "# **ðŸ”¹ Generate Synthetic Attack Samples with Multi-Class Labels**\n",
        "noise_input = np.random.uniform(-1.0, 1.0, size=[dim, 100])  # Random noise for generator\n",
        "\n",
        "# Define multi-class labels (5 attack categories)\n",
        "class_labels = np.random.randint(0, 5, size=(dim,))  # Random attack category assignment\n",
        "\n",
        "# One-hot encode labels (for ACGAN)\n",
        "noise_label = np.zeros((dim, 5))  # Change to 5 classes\n",
        "noise_label[np.arange(dim), class_labels] = 1  # One-hot encoding\n",
        "\n",
        "# Create input for generator\n",
        "noise_input = [noise_input, noise_label]\n",
        "\n",
        "# **ðŸ”¹ Generate Synthetic Attack Data**\n",
        "generator.summary()\n",
        "predictions = generator.predict(noise_input)\n",
        "predictions = tf.reshape(predictions, [dim, 10, 10])\n",
        "\n",
        "print(\"Generated Samples Shape:\", predictions.shape)\n",
        "\n",
        "# Convert predictions to list\n",
        "new_samples = predictions.numpy().tolist()\n",
        "\n",
        "# **ðŸ”¹ Add Generated Data to Training Set**\n",
        "Xtrain = list(Xtrain)  # Convert to list\n",
        "Xtrain.extend(new_samples)  # Add new generated samples\n",
        "\n",
        "# Convert Ytrain to DataFrame for easy concatenation\n",
        "Ytrain = pd.Series(Ytrain)\n",
        "\n",
        "# Append **multi-class attack labels** instead of just 1s\n",
        "Ytrain = pd.concat([Ytrain, pd.Series(class_labels)], ignore_index=True)\n",
        "\n",
        "print(\"Updated Training Data Shape:\", len(Xtrain))\n",
        "print(\"Updated Labels Shape:\", len(Ytrain))\n",
        "\n",
        "# Save updated datasets\n",
        "with open(param[\"dir\"] + 'XTrain50A%.pickle', 'wb') as f:\n",
        "    pickle.dump(Xtrain, f)\n",
        "\n",
        "with open(param[\"dir\"] + 'YTrain50A%.pickle', 'wb') as f:\n",
        "    pickle.dump(Ytrain, f)\n",
        "\n",
        "print(\"âœ… Multi-Class Synthetic Attack Samples Added Successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08zobPjn_4un"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Function to count normal and attack samples in a pickle label file\n",
        "def count_labels():\n",
        "\n",
        "    xtrain50A_pickle_path = \"D:/IDS/Code/Datasets/NSLKDD/XTrain50A%.pickle\"\n",
        "    ytrain50A_pickle_path = \"D:/IDS/Code/Datasets/NSLKDD/YTrain50A%.pickle\"\n",
        "    try:\n",
        "        with open(xtrain50A_pickle_path, \"rb\") as f:\n",
        "            xtrain = pickle.load(f)\n",
        "        with open(ytrain50A_pickle_path, \"rb\") as f:\n",
        "            ytrain = pickle.load(f)\n",
        "          # Convert to Pandas Series (if not already)\n",
        "        normal_count = (ytrain == 0).sum()\n",
        "        attack_count = (ytrain == 1).sum()\n",
        "\n",
        "        print(f\" - Normal Samples: {normal_count}\")\n",
        "        print(f\" - Attack Samples: {attack_count}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading : {e}\")\n",
        "\n",
        "# Count labels in the pickle files\n",
        "count_labels()\n",
        "train_csv_path = \"D:/IDS/Code/Datasets/NSL_KDD_Train.csv\"\n",
        "\n",
        "# Count labels in the Train.csv file\n",
        "try:\n",
        "    df = pd.read_csv(train_csv_path)\n",
        "    class_column = \"classification\"  # Ensure this matches exactly with Train.csv\n",
        "    normal_count = (df[class_column] == 0).sum()\n",
        "    attack_count = (df[class_column] == 1).sum()\n",
        "    print(f\"File: {train_csv_path}\")\n",
        "    print(f\" - Normal Samples: {normal_count}\")\n",
        "    print(f\" - Attack Samples: {attack_count}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading {train_csv_path}: {e}\")\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load trained ACGAN model\n",
        "pathModel = 'D:/IDS/Code/Datasets/NSLKDD/acgan_aagm.h5'\n",
        "generator = load_model(pathModel, compile=False)\n",
        "\n",
        "# Load dataset\n",
        "param = {\"dir\": \"D:/IDS/Code/Datasets/NSLKDD/\", \"Max_A_Size\": 10, \"Max_B_Size\": 10, \"mutual_info\": True}\n",
        "method = 'MI' if param['mutual_info'] else 'Mean'\n",
        "\n",
        "with open(param[\"dir\"] + f'train_{param[\"Max_A_Size\"]}x{param[\"Max_B_Size\"]}_{method}.pickle', 'rb') as f:\n",
        "    Xtrain = pickle.load(f)\n",
        "\n",
        "with open(param[\"dir\"] + 'Ytrain.pickle', 'rb') as f:\n",
        "    Ytrain = pickle.load(f)\n",
        "\n",
        "Xtrain = np.array(Xtrain)\n",
        "Ytrain = pd.Series(Ytrain)\n",
        "\n",
        "# ðŸ”¹ Augment R2L (Category 3)\n",
        "dim_r2l = 5000\n",
        "noise_input_r2l = np.random.uniform(-1.0, 1.0, size=[dim_r2l, 100])\n",
        "noise_label_r2l = np.zeros((dim_r2l, 5))\n",
        "noise_label_r2l[:, 3] = 1\n",
        "\n",
        "generated_r2l = generator.predict([noise_input_r2l, noise_label_r2l])\n",
        "generated_r2l = tf.reshape(generated_r2l, [dim_r2l, 10, 10]).numpy().tolist()\n",
        "\n",
        "# ðŸ”¹ Augment U2R (Category 4)\n",
        "dim_u2r = 5000\n",
        "noise_input_u2r = np.random.uniform(-1.0, 1.0, size=[dim_u2r, 100])\n",
        "noise_label_u2r = np.zeros((dim_u2r, 5))\n",
        "noise_label_u2r[:, 4] = 1\n",
        "\n",
        "generated_u2r = generator.predict([noise_input_u2r, noise_label_u2r])\n",
        "generated_u2r = tf.reshape(generated_u2r, [dim_u2r, 10, 10]).numpy().tolist()\n",
        "\n",
        "# Add new samples to training set\n",
        "Xtrain = list(Xtrain)\n",
        "Xtrain.extend(generated_r2l)\n",
        "Xtrain.extend(generated_u2r)\n",
        "\n",
        "# Append new labels\n",
        "Ytrain = pd.concat([Ytrain, pd.Series([3] * dim_r2l)], ignore_index=True)\n",
        "Ytrain = pd.concat([Ytrain, pd.Series([4] * dim_u2r)], ignore_index=True)\n",
        "\n",
        "print(\"âœ… Updated Training Data Shape:\", len(Xtrain))\n",
        "print(\"âœ… Updated Labels Shape:\", len(Ytrain))\n",
        "\n",
        "# Save updated dataset\n",
        "with open(param[\"dir\"] + 'XTrain50A%.pickle', 'wb') as f:\n",
        "    pickle.dump(Xtrain, f)\n",
        "\n",
        "with open(param[\"dir\"] + 'YTrain50A%.pickle', 'wb') as f:\n",
        "    pickle.dump(Ytrain, f)\n",
        "\n",
        "print(\"âœ… Multi-Class Augmentation for R2L & U2R Completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N77osjLM8AcX",
        "outputId": "2b8acb14-6b67-4826-ee47-ac12b44e3dbc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Function to count normal and attack samples in a pickle label file\n",
        "def count_labels():\n",
        "\n",
        "    xtrain50A_pickle_path = \"D:/IDS/Code/Datasets/NSLKDD/test_10x10_MI.pickle\"\n",
        "    ytrain50A_pickle_path = \"Datasets/NSLKDD/YTrain.pickle\"\n",
        "    try:\n",
        "        with open(xtrain50A_pickle_path, \"rb\") as f:\n",
        "            xtrain = pickle.load(f)\n",
        "        with open(ytrain50A_pickle_path, \"rb\") as f:\n",
        "            ytrain = pickle.load(f)\n",
        "          # Convert to Pandas Series (if not already)\n",
        "        normal_count = (ytrain == 0).sum()\n",
        "        attack_count = (ytrain == 3).sum()\n",
        "\n",
        "        print(f\" - Normal Samples: {normal_count}\")\n",
        "        print(f\" - Attack Samples: {attack_count}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading : {e}\")\n",
        "\n",
        "# Count labels in the pickle files\n",
        "count_labels()\n",
        "train_csv_path = \"D:/IDS/Code/Datasets/NSL_KDD_Train.csv\"\n",
        "\n",
        "# Count labels in the Train.csv file\n",
        "try:\n",
        "    df = pd.read_csv(train_csv_path)\n",
        "    class_column = \"classification\"  # Ensure this matches exactly with Train.csv\n",
        "    normal_count = (df[class_column] == 0).sum()\n",
        "    attack_count = (df[class_column] == 1).sum()\n",
        "    print(f\"File: {train_csv_path}\")\n",
        "    print(f\" - Normal Samples: {normal_count}\")\n",
        "    print(f\" - Attack Samples: {attack_count}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading {train_csv_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46CzK7zgbrQh"
      },
      "source": [
        "# Main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "OTtzoSZvuoUl",
        "outputId": "8a258f25-45fd-484d-c92f-d407fc037b2a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Parameters\n",
        "param = {\"Max_A_Size\": 10, \"Max_B_Size\": 10, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180,\n",
        "          \"dir\": \"D:/IDS/Code/Datasets/\", \"Mode\": \"CNN2\",  # Mode : CNN_Nature, CNN2    | DIR: local working directory\n",
        "          \"res\":\"D:/IDS/Code/Results/\",#remote path for saving data\n",
        "          \"LoadFromJson\": True, \"mutual_info\": True, # LoadFromJson: True: load pickle files (images)\n",
        "          #MOVE Ytrain.pickle and Ytest.pickle to the working directory after generating the images\n",
        "          \"hyper_opt_evals\": 20, \"epoch\": 150, \"No_0_MI\": False,  # True -> Removing 0 MI Features\n",
        "          \"autoencoder\": False,  \"cut\": None,\"enhanced_dataset\": \"gan\" # gan, smote, adasyn, \"\"None\"\"\n",
        "          }\n",
        "\n",
        "\n",
        "\n",
        "dataset = 3  # change dataset\n",
        "if dataset == 1:\n",
        "    train = 'TrainOneCls.csv'\n",
        "    test = 'Test.csv'\n",
        "    classif_label = 'Classification'\n",
        "    param[\"attack_label\"] = 0\n",
        "elif dataset == 2:\n",
        "    train = 'Train.csv'\n",
        "    test = 'Test_UNSW_NB15.csv'\n",
        "    classif_label = 'classification'\n",
        "    param[\"attack_label\"] = 1\n",
        "elif dataset == 3:\n",
        "    train = 'Train.csv'\n",
        "    test = 'Test.csv'\n",
        "    classif_label = ' classification.'\n",
        "    param[\"attack_label\"] = 1\n",
        "elif dataset == 4:\n",
        "    train = 'AAGMTrain_OneClsNumeric.csv'\n",
        "    test = 'AAGMTest_OneClsNumeric.csv'\n",
        "    classif_label = 'classification'\n",
        "    param[\"attack_label\"] = 0\n",
        "\n",
        "\n",
        "if not param[\"LoadFromJson\"]:\n",
        "    data = {}\n",
        "    with open(param[\"dir\"] + train, 'r') as file:\n",
        "        data = {\"Xtrain\": pd.DataFrame(list(csv.DictReader(file))).apply(pd.to_numeric, errors='coerce'), \"class\": 2}\n",
        "        data[\"Classification\"] = data[\"Xtrain\"][classif_label]\n",
        "        del data[\"Xtrain\"][classif_label]\n",
        "    with open(param[\"dir\"]+test, 'r') as file:\n",
        "        Xtest = pd.DataFrame(list(csv.DictReader(file)))\n",
        "        #Xtest.drop(Xtest.keys()[0], axis=1)\n",
        "        Xtest.replace(\"\", np.nan, inplace=True)\n",
        "        Xtest.dropna(inplace=True)\n",
        "        data[\"Xtest\"] = Xtest[Xtest.keys()[1:]].apply(pd.to_numeric, errors='coerce')\n",
        "        data[\"Ytest\"] = data[\"Xtest\"][classif_label]\n",
        "        del data[\"Xtest\"][classif_label]\n",
        "\n",
        "        filename = \"work/Ytrain.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(data[\"Classification\"], f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = \"Ytest.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(data[\"Ytest\"], f_myfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        f_myfile.close()\n",
        "\n",
        "        if param[\"enhanced_dataset\"] == \"smote\":\n",
        "          sm = SMOTE(random_state=42)\n",
        "          data[\"Xtrain\"], data[\"Classification\"] = sm.fit_resample(data[\"Xtrain\"], data[\"Classification\"])\n",
        "        elif param[\"enhanced_dataset\"] == \"adasyn\":\n",
        "          ada = ADASYN(random_state=42)\n",
        "          data[\"Xtrain\"], data[\"Classification\"] = ada.fit_resample(data[\"Xtrain\"], data[\"Classification\"])\n",
        "\n",
        "\n",
        "     # AUTOENCODER\n",
        "    if param[\"autoencoder\"]:\n",
        "        autoencoder = load_model(param[\"dir\"] + 'Autoencoder.h5')\n",
        "        autoencoder.summary()\n",
        "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encod2').output)\n",
        "        encoder.summary()\n",
        "        # usa l'encoder con predict sul train_X e poi su test_X. Io qui ho creato anche il dataframe per salvarlo poi come csv\n",
        "        encoded_train = pd.DataFrame(encoder.predict(data[\"Xtrain\"]))\n",
        "        data[\"Xtrain\"] = encoded_train.add_prefix('feature_')\n",
        "        encoded_test = pd.DataFrame(encoder.predict(data[\"Xtest\"]))\n",
        "        data[\"Xtest\"] = encoded_test.add_prefix('feature_')\n",
        "\n",
        "\n",
        "    model = train_norm(param, data, norm=False)\n",
        "\n",
        "else:\n",
        "    images = {}\n",
        "    if param['mutual_info']:\n",
        "        method = 'MI'\n",
        "    else:\n",
        "        method = 'Mean'\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'train_'+str(param['Max_A_Size'])+'x'+str(param['Max_B_Size'])+'_'+method+'.pickle', 'rb')\n",
        "    images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTrain.pickle', 'rb')\n",
        "    images[\"Classification\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'test_'+str(param['Max_A_Size'])+'x'+str(param['Max_B_Size'])+'_'+method+'.pickle', 'rb')\n",
        "    images[\"Xtest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTest.pickle', 'rb')\n",
        "    images[\"Ytest\"] = pd.read_pickle(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    if param[\"enhanced_dataset\"] == \"gan\":\n",
        "\n",
        "        new=predictions.numpy()\n",
        "        print(len(new))\n",
        "        images[\"Xtrain\"].extend(new)\n",
        "        print(len(images[\"Xtrain\"]))\n",
        "\n",
        "        images[\"Classification\"]=images[\"Classification\"].append(pd.Series(np.zeros(dim)))\n",
        "        #.append(list(np.zeros(dim)))\n",
        "        #print(images[\"Xtrain\"])\n",
        "\n",
        "    #print(4,len(images[\"Ytest\"]))\n",
        "    filename = param[\"res\"] + \"trainGAN.pickle\"\n",
        "    f_myfile = open(filename, 'wb')\n",
        "    pickle.dump(images, f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    model = train_norm(param, images, norm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBs-1HS_eMDn",
        "outputId": "6c1ae66c-33f0-44d6-bde0-e013c625a42e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "param = {\n",
        "    \"Max_A_Size\": 10,\n",
        "    \"Max_B_Size\": 10,\n",
        "    \"Dynamic_Size\": False,\n",
        "    'Metod': 'tSNE',\n",
        "    \"ValidRatio\": 0.1,\n",
        "    \"seed\": 180,\n",
        "    \"dir\": \"/content/IDS/Datasets/NSL_KDD/\",  # Directory for input files\n",
        "    \"res\": \"/content/IDS/Results/MultiClass/\",  # Directory for output results\n",
        "    \"Mode\": \"CNN2\",  # Model type\n",
        "    \"LoadFromJson\": True,  # Whether to load preprocessed images\n",
        "    \"mutual_info\": True,  # Method: Mutual Information (True) or Mean (False)\n",
        "    \"hyper_opt_evals\": 20,\n",
        "    \"epoch\": 150,\n",
        "    \"No_0_MI\": False,  # Whether to remove features with 0 MI\n",
        "    \"autoencoder\": False,\n",
        "    \"cut\": None,\n",
        "    \"enhanced_dataset\": None  # Options: 'gan', 'smote', 'adasyn', None\n",
        "}\n",
        "\n",
        "# Set paths\n",
        "best_model_path = \"/content/IDS/Datasets/NSL_KDD/nsl_kdd_best_model.keras\"  # Path to the trained model\n",
        "test_pickle_path = param[\"dir\"] + f'test_{param[\"Max_A_Size\"]}x{param[\"Max_B_Size\"]}_{\"MI\" if param[\"mutual_info\"] else \"Mean\"}.pickle'\n",
        "test_labels_path = param[\"dir\"] + \"Ytest.pickle\"\n",
        "\n",
        "# Load Model\n",
        "print(\"Loading trained model...\")\n",
        "model = load_model(best_model_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Check for preprocessed test data\n",
        "try:\n",
        "    print(\"Attempting to load preprocessed test data...\")\n",
        "    with open(test_pickle_path, 'rb') as f:\n",
        "        XTestGlobal = pickle.load(f)\n",
        "    with open(test_labels_path, 'rb') as f:\n",
        "        YTestGlobal = pd.read_pickle(f)\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    print(\"Preprocessed test data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "\n",
        "    print(\"Preprocessed test data not found. Regenerating from raw test.csv...\")\n",
        "\n",
        "    # Load raw test data\n",
        "    test_csv_path = param[\"dir\"] + \"NSL_KDD_Test.csv\"\n",
        "    test_data = pd.read_csv(test_csv_path)\n",
        "    test_data.replace(\"\", np.nan, inplace=True)\n",
        "    test_data.dropna(inplace=True)\n",
        "\n",
        "    # Extract labels and features\n",
        "    YTestGlobal = test_data[\"classification\"].astype(int)\n",
        "    del test_data[\"classification\"]\n",
        "    XTestGlobal_raw = test_data.values\n",
        "\n",
        "    # Normalize if needed\n",
        "    print(\"Normalizing test data...\")\n",
        "    test_max, test_min = XTestGlobal_raw.max(), XTestGlobal_raw.min()\n",
        "    XTestGlobal_raw = (XTestGlobal_raw - test_min) / (test_max - test_min)\n",
        "\n",
        "    # Reshape using Cart2Pixel or ConvPixel\n",
        "    print(\"Generating 2D images from raw test data...\")\n",
        "    _, image_model, toDelete = Cart2Pixel(\n",
        "        {\"data\": XTestGlobal_raw.T, \"method\": param[\"Metod\"], \"max_A_size\": param[\"Max_A_Size\"], \"max_B_size\": param[\"Max_B_Size\"]},\n",
        "        param[\"Max_A_Size\"],\n",
        "        param[\"Max_B_Size\"],\n",
        "        param[\"Dynamic_Size\"],\n",
        "        mutual_info=param[\"mutual_info\"],\n",
        "        params=param\n",
        "    )\n",
        "\n",
        "    XTestGlobal = [\n",
        "        ConvPixel(\n",
        "            XTestGlobal_raw[:, i],\n",
        "            np.array(image_model[\"xp\"]),\n",
        "            np.array(image_model[\"yp\"]),\n",
        "            image_model[\"A\"],\n",
        "            image_model[\"B\"],\n",
        "            custom_cut=image_model[\"custom_cut\"] if image_model[\"custom_cut\"] else None\n",
        "        ) for i in range(XTestGlobal_raw.shape[1])\n",
        "    ]\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "\n",
        "    # Save preprocessed test data for future use\n",
        "    print(\"Saving preprocessed test data...\")\n",
        "    with open(test_pickle_path, 'wb') as f:\n",
        "        pickle.dump(XTestGlobal, f)\n",
        "    with open(test_labels_path, 'wb') as f:\n",
        "        pickle.dump(YTestGlobal, f)\n",
        "\n",
        "# Reshape test data for the model\n",
        "print(\"Preparing test data for prediction...\")\n",
        "\n",
        "image_size1, image_size2 = XTestGlobal.shape[1], XTestGlobal.shape[2]\n",
        "XTestGlobal = np.reshape(XTestGlobal, (-1, image_size1, image_size2, 1))\n",
        "\n",
        "# Convert test labels to categorical (one-hot encoding)\n",
        "num_classes = 5  # Number of attack categories (Normal, DoS, Probe, R2L, U2R)\n",
        "YTestGlobal = to_categorical(YTestGlobal, num_classes=num_classes)\n",
        "\n",
        "# Predict using the trained model\n",
        "print(\"Making predictions on test data...\")\n",
        "Y_predicted_probs = model.predict(XTestGlobal, verbose=1)\n",
        "Y_predicted = np.argmax(Y_predicted_probs, axis=1)\n",
        "YTestGlobal = np.argmax(YTestGlobal, axis=1)  # Convert one-hot back to labels\n",
        "\n",
        "# Evaluate the results\n",
        "print(\"Evaluating predictions...\")\n",
        "accuracy = balanced_accuracy_score(YTestGlobal, Y_predicted)\n",
        "conf_matrix = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "\n",
        "# **ðŸ”¹ Calculate Per-Class Accuracy**\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "total_correct = conf_matrix.diagonal().sum()  # Sum of correctly classified samples\n",
        "total_samples = conf_matrix.sum()  # Total number of samples\n",
        "overall_accuracy = total_correct / total_samples\n",
        "\n",
        "print(f\"\\nðŸ”¹ **Overall Accuracy**: {overall_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(f\"Balanced Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
        "\n",
        "# Print per-class accuracy\n",
        "print(\"\\nðŸ”¹ **Accuracy for Each Attack Category**:\")\n",
        "attack_categories = [\"Normal\", \"DoS\", \"Probe\", \"R2L\",\"U2R\"]\n",
        "for i, category in enumerate(attack_categories):\n",
        "    print(f\"{category}: {class_accuracies[i] * 100:.2f}%\")\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(YTestGlobal, Y_predicted, target_names=attack_categories)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XAI Multiclass Better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, classification_report\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Step 1: Define NSL-KDD feature names\n",
        "def get_nsl_kdd_feature_names():\n",
        "    \"\"\"Return the list of feature names for the NSL-KDD dataset\"\"\"\n",
        "    return [\n",
        "        \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\",\n",
        "        \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n",
        "        \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
        "        \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\",\n",
        "        \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
        "        \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "        \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "        \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "        \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"\n",
        "    ]\n",
        "\n",
        "# Feature descriptions for network security context\n",
        "def get_feature_descriptions():\n",
        "    \"\"\"Return descriptions of the NSL-KDD features in a network security context\"\"\"\n",
        "    return {\n",
        "        \"duration\": \"Length of the connection in seconds\",\n",
        "        \"protocol_type\": \"Type of protocol (e.g., tcp, udp, icmp)\",\n",
        "        \"service\": \"Network service on destination (e.g., http, ftp)\",\n",
        "        \"flag\": \"Normal or error status of the connection\",\n",
        "        \"src_bytes\": \"Bytes sent from source to destination\",\n",
        "        \"dst_bytes\": \"Bytes sent from destination to source\",\n",
        "        \"land\": \"1 if connection is from/to same host/port; 0 otherwise\",\n",
        "        \"wrong_fragment\": \"Number of 'wrong' fragments\",\n",
        "        \"urgent\": \"Number of urgent packets\",\n",
        "        \"hot\": \"Number of 'hot' indicators (sensitive actions like accessing system files)\",\n",
        "        \"num_failed_logins\": \"Number of failed login attempts\",\n",
        "        \"logged_in\": \"1 if successfully logged in; 0 otherwise\",\n",
        "        \"num_compromised\": \"Number of 'compromised' conditions\",\n",
        "        \"root_shell\": \"1 if root shell is obtained; 0 otherwise\",\n",
        "        \"su_attempted\": \"1 if 'su root' command attempted; 0 otherwise\",\n",
        "        \"num_root\": \"Number of 'root' accesses\",\n",
        "        \"num_file_creations\": \"Number of file creation operations\",\n",
        "        \"num_shells\": \"Number of shell prompts\",\n",
        "        \"num_access_files\": \"Number of sensitive file access operations\",\n",
        "        \"num_outbound_cmds\": \"Number of outbound commands in an ftp session\",\n",
        "        \"is_host_login\": \"1 if the login belongs to the 'hot' list; 0 otherwise\",\n",
        "        \"is_guest_login\": \"1 if the login is a 'guest' login; 0 otherwise\",\n",
        "        \"count\": \"Number of connections to the same host in past 2 seconds\",\n",
        "        \"srv_count\": \"Number of connections to the same service in past 2 seconds\",\n",
        "        \"serror_rate\": \"% of connections with SYN errors\",\n",
        "        \"srv_serror_rate\": \"% of connections with SYN errors to the same service\",\n",
        "        \"rerror_rate\": \"% of connections with REJ errors\",\n",
        "        \"srv_rerror_rate\": \"% of connections with REJ errors to the same service\",\n",
        "        \"same_srv_rate\": \"% of connections to the same service\",\n",
        "        \"diff_srv_rate\": \"% of connections to different services\",\n",
        "        \"srv_diff_host_rate\": \"% of connections to different hosts\",\n",
        "        \"dst_host_count\": \"Count of connections to same destination host\",\n",
        "        \"dst_host_srv_count\": \"Count of connections to same destination host service\",\n",
        "        \"dst_host_same_srv_rate\": \"% of connections to same destination host service\",\n",
        "        \"dst_host_diff_srv_rate\": \"% of connections to different destination host services\",\n",
        "        \"dst_host_same_src_port_rate\": \"% of connections from same source port\",\n",
        "        \"dst_host_srv_diff_host_rate\": \"% of connections to same service from different hosts\",\n",
        "        \"dst_host_serror_rate\": \"% of SYN error connections to destination host\",\n",
        "        \"dst_host_srv_serror_rate\": \"% of SYN error connections to destination host service\",\n",
        "        \"dst_host_rerror_rate\": \"% of REJ error connections to destination host\",\n",
        "        \"dst_host_srv_rerror_rate\": \"% of REJ error connections to destination host service\"\n",
        "    }\n",
        "\n",
        "# Step 2: Update image model with feature names function\n",
        "def update_image_model_with_features(image_model_path, param, feature_names=None):\n",
        "    \"\"\" Update image model with feature names if they are missing \"\"\"\n",
        "    \n",
        "    # Use NSL-KDD features if not provided\n",
        "    if feature_names is None:\n",
        "        feature_names = get_nsl_kdd_feature_names()\n",
        "    \n",
        "    # Load existing image model if available\n",
        "    if os.path.exists(image_model_path):\n",
        "        with open(image_model_path, 'rb') as f:\n",
        "            image_model = pickle.load(f)\n",
        "    else:\n",
        "        # Create a basic image model structure\n",
        "        image_model = {\n",
        "            \"xp\": None,\n",
        "            \"yp\": None,\n",
        "            \"A\": param[\"Max_A_Size\"],\n",
        "            \"B\": param[\"Max_B_Size\"],\n",
        "            \"custom_cut\": None\n",
        "        }\n",
        "    \n",
        "    # Add or update feature mapping\n",
        "    if 'feature_mapping' not in image_model:\n",
        "        image_model['feature_mapping'] = {\n",
        "            'pixel_mapping': {},\n",
        "            'feature_names': feature_names\n",
        "        }\n",
        "        \n",
        "        # Create basic pixel mapping based on Cart2Pixel transformation\n",
        "        for feature_idx, feature_name in enumerate(feature_names):\n",
        "            if feature_idx < param[\"Max_A_Size\"] * param[\"Max_B_Size\"]:\n",
        "                x = feature_idx % param[\"Max_A_Size\"]\n",
        "                y = feature_idx // param[\"Max_A_Size\"]\n",
        "                pixel_idx = y * param[\"Max_A_Size\"] + x\n",
        "                \n",
        "                image_model['feature_mapping']['pixel_mapping'][str(pixel_idx)] = {\n",
        "                    'feature_name': feature_name,\n",
        "                    'x_coord': x + 1,\n",
        "                    'y_coord': y + 1\n",
        "                }\n",
        "    \n",
        "    # Save updated image model\n",
        "    with open(image_model_path, 'wb') as f:\n",
        "        pickle.dump(image_model, f)\n",
        "    \n",
        "    return image_model\n",
        "\n",
        "# Define attack characteristics for better explanations\n",
        "def get_attack_characteristics():\n",
        "    \"\"\" Return characteristics of different attack types \"\"\"\n",
        "    return {\n",
        "        \"Normal\": \"Regular, benign network traffic with expected patterns and behaviors.\",\n",
        "        \"DoS\": \"Denial of Service attacks attempt to make a network resource unavailable by flooding it with traffic or making it crash. Characterized by high volume of traffic to specific services.\",\n",
        "        \"Probe\": \"Scanning and probing attacks attempt to gather information about the network to identify vulnerabilities. Characterized by connections to many ports/services in short time.\",\n",
        "        \"R2L\": \"Remote to Local attacks attempt to gain unauthorized access from a remote machine. Characterized by unusual access patterns and credential manipulation.\",\n",
        "        \"U2R\": \"User to Root attacks attempt to gain root/admin privileges. Characterized by privilege escalation actions after gaining normal user access.\"\n",
        "    }\n",
        "\n",
        "# Helper function to create default image model\n",
        "def create_default_image_model(shape):\n",
        "    \"\"\"Creates a default image model with basic feature mapping\"\"\"\n",
        "    image_size1, image_size2 = shape\n",
        "    original_features = get_nsl_kdd_feature_names()\n",
        "    \n",
        "    image_model = {\n",
        "        \"xp\": None,\n",
        "        \"yp\": None,\n",
        "        \"A\": image_size1,\n",
        "        \"B\": image_size2,\n",
        "        \"custom_cut\": None,\n",
        "        \"feature_mapping\": {\n",
        "            'pixel_mapping': {},\n",
        "            'feature_names': original_features[:image_size1 * image_size2]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Create pixel mapping\n",
        "    for i in range(min(len(original_features), image_size1 * image_size2)):\n",
        "        x = i % image_size1\n",
        "        y = i // image_size1\n",
        "        pixel_idx = y * image_size1 + x\n",
        "        image_model['feature_mapping']['pixel_mapping'][str(pixel_idx)] = {\n",
        "            'feature_name': original_features[i],\n",
        "            'x_coord': x + 1,\n",
        "            'y_coord': y + 1\n",
        "        }\n",
        "    \n",
        "    return image_model\n",
        "\n",
        "# Helper function to map pixels to features\n",
        "def map_pixels_to_features(flat_shap_values, image_model, num_features):\n",
        "    \"\"\" Maps flattened SHAP values to features using image_model \"\"\"\n",
        "    image_size1 = image_model['A'] if 'A' in image_model else 10\n",
        "    image_size2 = image_model['B'] if 'B' in image_model else 10\n",
        "    \n",
        "    feature_importance = {}\n",
        "    \n",
        "    # Process each pixel's SHAP value\n",
        "    for pixel_idx, shap_value in enumerate(flat_shap_values):\n",
        "        # Skip if value is close to zero (no impact)\n",
        "        if abs(shap_value) < 1e-10:\n",
        "            continue\n",
        "            \n",
        "        # Map pixel to feature\n",
        "        pixel_key = str(pixel_idx)\n",
        "        if 'feature_mapping' in image_model and 'pixel_mapping' in image_model['feature_mapping'] and pixel_key in image_model['feature_mapping']['pixel_mapping']:\n",
        "            feature_info = image_model['feature_mapping']['pixel_mapping'][pixel_key]\n",
        "            feature_name = feature_info['feature_name']\n",
        "            \n",
        "            # Add or update feature importance\n",
        "            if feature_name not in feature_importance:\n",
        "                feature_importance[feature_name] = {\n",
        "                    'importance': float(shap_value),\n",
        "                    'abs_importance': abs(float(shap_value)),\n",
        "                    'coordinates': (feature_info['x_coord'], feature_info['y_coord']),\n",
        "                    'type': 'positive' if shap_value > 0 else 'negative'\n",
        "                }\n",
        "            else:\n",
        "                # If multiple pixels map to the same feature, use the one with the highest absolute value\n",
        "                if abs(shap_value) > feature_importance[feature_name]['abs_importance']:\n",
        "                    feature_importance[feature_name]['importance'] = float(shap_value)\n",
        "                    feature_importance[feature_name]['abs_importance'] = abs(float(shap_value))\n",
        "                    feature_importance[feature_name]['type'] = 'positive' if shap_value > 0 else 'negative'\n",
        "    \n",
        "    # Sort features by absolute importance\n",
        "    sorted_features = sorted(\n",
        "        feature_importance.items(),\n",
        "        key=lambda x: x[1]['abs_importance'],\n",
        "        reverse=True\n",
        "    )[:num_features]  # Get top N features\n",
        "    \n",
        "    return sorted_features\n",
        "\n",
        "# Function to directly analyze SHAP values for multi-class models\n",
        "def analyze_direct_shap(shap_values, X_sample, image_model, attack_categories, \n",
        "                        feature_descriptions, attack_chars, y_pred, y_true, \n",
        "                        num_features=15, output_dir=None):\n",
        "    \"\"\"Analyze SHAP values when they're provided in the (samples, h, w, channels, num_classes) format\"\"\"\n",
        "    print(f\"SHAP values shape: {shap_values.shape}\")\n",
        "    image_size1, image_size2 = X_sample.shape[1], X_sample.shape[2]\n",
        "    \n",
        "    # Results dictionary\n",
        "    class_analyses = {}\n",
        "    \n",
        "    for class_idx, class_name in enumerate(attack_categories):\n",
        "        is_predicted = (class_idx == y_pred)\n",
        "        is_true = (class_idx == y_true)\n",
        "        \n",
        "        status = \"\"\n",
        "        if is_predicted and is_true:\n",
        "            status = \"ðŸŸ¢ PREDICTED & TRUE CLASS\"\n",
        "        elif is_predicted:\n",
        "            status = \"ðŸŸ¡ PREDICTED CLASS\"\n",
        "        elif is_true:\n",
        "            status = \"ðŸ”µ TRUE CLASS\"\n",
        "            \n",
        "        print(f\"\\nðŸ”¹ SHAP Analysis for Class: {class_name} {status}\")\n",
        "        print(f\"Description: {attack_chars[class_name]}\")\n",
        "        \n",
        "        # Extract SHAP values for this class\n",
        "        # For multi-dimensional SHAP values shape (1, h, w, 1, num_classes)\n",
        "        class_shap = shap_values[..., class_idx]\n",
        "        \n",
        "        # Remove singleton dimensions\n",
        "        class_shap = np.squeeze(class_shap)\n",
        "        print(f\"Class SHAP shape after squeezing: {class_shap.shape}\")\n",
        "        \n",
        "        # Display class-specific SHAP heatmap\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        # FIX: Use vmin/vmax instead of center for plt.imshow\n",
        "        abs_max = np.max(np.abs(class_shap))\n",
        "        plt.imshow(class_shap, cmap='RdBu_r', vmin=-abs_max, vmax=abs_max)\n",
        "        plt.colorbar(label='SHAP Value')\n",
        "        plt.title(f\"SHAP Values for {class_name} {status}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Flatten SHAP values for feature mapping\n",
        "        flat_shap = class_shap.flatten()\n",
        "        \n",
        "        # Map pixels to features\n",
        "        sorted_features = map_pixels_to_features(flat_shap, image_model, num_features)\n",
        "        \n",
        "        if not sorted_features:\n",
        "            print(\"No significant feature importance values found for this class.\")\n",
        "            continue\n",
        "        \n",
        "        # Create visualization\n",
        "        plt.figure(figsize=(18, 8))\n",
        "        \n",
        "        # 1. Feature Importance Plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        features = [x[0] for x in sorted_features]\n",
        "        importance_values = [x[1]['importance'] for x in sorted_features]\n",
        "        colors = ['red' if x[1]['type'] == 'positive' else 'blue' for x in sorted_features]\n",
        "        \n",
        "        # Create horizontal bar plot\n",
        "        bars = plt.barh(range(len(features)), importance_values, color=colors)\n",
        "        \n",
        "        # Only add values to the bars if we have importance values\n",
        "        if importance_values:\n",
        "            max_importance = max([abs(x) for x in importance_values])\n",
        "            for i, bar in enumerate(bars):\n",
        "                value = importance_values[i]\n",
        "                if value > 0:\n",
        "                    plt.text(value + max_importance * 0.01, i, f\"{value:.2e}\", va='center')\n",
        "                else:\n",
        "                    plt.text(value - max_importance * 0.05, i, f\"{value:.2e}\", va='center', ha='right', color='white')\n",
        "        \n",
        "        plt.yticks(range(len(features)), [f\"{f[:25]}...\" if len(f) > 25 else f for f in features])\n",
        "        plt.xlabel('SHAP Value')\n",
        "        plt.title(f'Top {num_features} Feature Contributions for {class_name}')\n",
        "        \n",
        "        # Add legend\n",
        "        red_patch = mpatches.Patch(color='red', label='Increases probability')\n",
        "        blue_patch = mpatches.Patch(color='blue', label='Decreases probability')\n",
        "        plt.legend(handles=[red_patch, blue_patch], loc='lower right')\n",
        "        \n",
        "        # 2. Pixel Location Plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        pixel_values = np.zeros((image_size1, image_size2))\n",
        "        \n",
        "        # Fill in pixel values from top features\n",
        "        for feature_name, details in sorted_features:\n",
        "            x, y = details['coordinates']\n",
        "            # Ensure coordinates are within bounds and adjust to 0-indexed\n",
        "            x = min(x-1, image_size1-1)\n",
        "            y = min(y-1, image_size2-1)\n",
        "            pixel_values[y, x] = details['importance']  # Note: y, x for array indexing\n",
        "        \n",
        "        # Create heatmap\n",
        "        sns.heatmap(pixel_values, cmap='RdBu_r', center=0)\n",
        "        plt.title(f'Feature Location Heatmap for {class_name}')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save the figure if output directory is provided\n",
        "        if output_dir:\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.makedirs(output_dir)\n",
        "            plt.savefig(os.path.join(output_dir, f'feature_importance_{class_name}.png'))\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "        # Create HTML table for feature importance with enhanced explanations\n",
        "        table_html = f\"\"\"\n",
        "        <h3>Top {min(num_features, len(sorted_features))} Features for {class_name} Classification {status}</h3>\n",
        "        <table style=\"width:100%; border-collapse: collapse; margin-bottom: 20px;\">\n",
        "            <tr style=\"background-color: #f2f2f2;\">\n",
        "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Rank</th>\n",
        "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Feature</th>\n",
        "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Description</th>\n",
        "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Importance</th>\n",
        "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Impact</th>\n",
        "            </tr>\n",
        "        \"\"\"\n",
        "        \n",
        "        for i, (feature_name, details) in enumerate(sorted_features):\n",
        "            color = \"#ffebee\" if details['type'] == 'positive' else \"#e3f2fd\"\n",
        "            impact = \"Increases probability\" if details['type'] == 'positive' else \"Decreases probability\"\n",
        "            description = feature_descriptions.get(feature_name, \"No description available\")\n",
        "            \n",
        "            table_html += f\"\"\"\n",
        "            <tr style=\"background-color: {color}\">\n",
        "                <td style=\"padding: 12px; border: 1px solid #ddd;\">{i+1}</td>\n",
        "                <td style=\"padding: 12px; border: 1px solid #ddd;\">{feature_name}</td>\n",
        "                <td style=\"padding: 12px; border: 1px solid #ddd;\">{description}</td>\n",
        "                <td style=\"padding: 12px; border: 1px solid #ddd;\">{details['abs_importance']:.2e}</td>\n",
        "                <td style=\"padding: 12px; border: 1px solid #ddd;\">{impact}</td>\n",
        "            </tr>\n",
        "            \"\"\"\n",
        "        \n",
        "        table_html += \"</table>\"\n",
        "        display(HTML(table_html))\n",
        "        \n",
        "        # Provide interpretation of the results\n",
        "        print(\"\\nðŸ”¹ Interpretation:\")\n",
        "        print(f\"For {class_name} classification, the model is:\")\n",
        "        \n",
        "        # Get top 3 positive and negative features if available\n",
        "        positive_features = [(name, details) for name, details in sorted_features if details['type'] == 'positive']\n",
        "        negative_features = [(name, details) for name, details in sorted_features if details['type'] == 'negative']\n",
        "        \n",
        "        # Sort by absolute importance\n",
        "        positive_features = sorted(positive_features, key=lambda x: x[1]['abs_importance'], reverse=True)[:3]\n",
        "        negative_features = sorted(negative_features, key=lambda x: x[1]['abs_importance'], reverse=True)[:3]\n",
        "        \n",
        "        if positive_features:\n",
        "            print(\"  Primarily focusing on (increasing probability):\")\n",
        "            for name, details in positive_features:\n",
        "                print(f\"    - {name}: {feature_descriptions.get(name, 'No description available')}\")\n",
        "        \n",
        "        if negative_features:\n",
        "            print(\"  Being discouraged by (decreasing probability):\")\n",
        "            for name, details in negative_features:\n",
        "                print(f\"    - {name}: {feature_descriptions.get(name, 'No description available')}\")\n",
        "        \n",
        "        print(f\"\\nThis {'' if is_predicted else 'would have '} led to a {class_name} classification.\")\n",
        "        \n",
        "        # Save class analysis\n",
        "        class_analyses[class_name] = {\n",
        "            'class_name': class_name,\n",
        "            'shap_values': class_shap,\n",
        "            'feature_importance': sorted_features,\n",
        "            'is_predicted': is_predicted,\n",
        "            'is_true': is_true\n",
        "        }\n",
        "    \n",
        "    return class_analyses\n",
        "\n",
        "# Main function for explainable AI\n",
        "def explain_multi_class_prediction(model_path, test_data_path, test_labels_path, image_model_path=None, \n",
        "                                  num_features=15, output_dir=None, sample_idx=None, explain_all_classes=True):\n",
        "    \"\"\"\n",
        "    Generate explanations for multi-class intrusion detection predictions.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_path : str\n",
        "        Path to the trained model file\n",
        "    test_data_path : str\n",
        "        Path to the test data pickle file\n",
        "    test_labels_path : str\n",
        "        Path to the test labels pickle file\n",
        "    image_model_path : str, optional\n",
        "        Path to the image model pickle file (for feature mapping)\n",
        "    num_features : int, default=15\n",
        "        Number of top features to display\n",
        "    output_dir : str, optional\n",
        "        Directory to save output files\n",
        "    sample_idx : int, optional\n",
        "        Index of specific sample to explain (random if None)\n",
        "    explain_all_classes : bool, default=True\n",
        "        Whether to explain all classes or just the predicted and true classes\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing explanation results\n",
        "    \"\"\"\n",
        "    # Attack category names and characteristics\n",
        "    attack_categories = [\"Normal\", \"DoS\", \"Probe\", \"R2L\", \"U2R\"]\n",
        "    attack_chars = get_attack_characteristics()\n",
        "    feature_descriptions = get_feature_descriptions()\n",
        "    \n",
        "    # Load model\n",
        "    print(\"Loading trained model...\")\n",
        "    model = load_model(model_path)\n",
        "    print(\"Model loaded successfully.\")\n",
        "    \n",
        "    # Load test data\n",
        "    print(\"Loading test data...\")\n",
        "    with open(test_data_path, 'rb') as f:\n",
        "        XTestGlobal = pickle.load(f)\n",
        "    with open(test_labels_path, 'rb') as f:\n",
        "        YTestGlobal = pd.read_pickle(f)\n",
        "    \n",
        "    # Convert to numpy array if needed\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    \n",
        "    # Try to load image_model for feature mapping\n",
        "    if image_model_path and os.path.exists(image_model_path):\n",
        "        print(\"Loading image model for feature mapping...\")\n",
        "        with open(image_model_path, 'rb') as f:\n",
        "            image_model = pickle.load(f)\n",
        "        print(\"Image model loaded successfully\")\n",
        "    else:\n",
        "        print(\"Image model not found. Creating default feature mapping...\")\n",
        "        image_model = create_default_image_model(XTestGlobal.shape[1:3])\n",
        "    \n",
        "    # Reshape test data for the model\n",
        "    image_size1, image_size2 = XTestGlobal.shape[1], XTestGlobal.shape[2]\n",
        "    XTestGlobal_reshaped = np.reshape(XTestGlobal, (-1, image_size1, image_size2, 1))\n",
        "    \n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    Y_predicted_probs = model.predict(XTestGlobal_reshaped, verbose=1)\n",
        "    Y_predicted = np.argmax(Y_predicted_probs, axis=1)\n",
        "    \n",
        "    # Display overall performance metrics\n",
        "    print(\"\\nðŸ”¹ Overall Performance Metrics:\")\n",
        "    accuracy = balanced_accuracy_score(YTestGlobal, Y_predicted)\n",
        "    conf_matrix = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "    print(f\"Balanced Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "    \n",
        "    # Display classification report\n",
        "    print(\"\\nðŸ”¹ Classification Report:\")\n",
        "    report = classification_report(YTestGlobal, Y_predicted, target_names=attack_categories)\n",
        "    print(report)\n",
        "    \n",
        "    # SHAP Analysis\n",
        "    print(\"\\nðŸ”¹ Generating SHAP explanations...\")\n",
        "    \n",
        "    # Use a subset of data as background for the explainer\n",
        "    background = XTestGlobal_reshaped[:20]  # Reduced from 50 to 20 samples to save memory\n",
        "    \n",
        "    # Select sample to explain (random if not specified)\n",
        "    if sample_idx is None:\n",
        "        sample_idx = np.random.choice(XTestGlobal_reshaped.shape[0])\n",
        "    \n",
        "    X_sample = XTestGlobal_reshaped[sample_idx:sample_idx+1]\n",
        "    y_true = YTestGlobal[sample_idx]\n",
        "    y_pred = Y_predicted[sample_idx]\n",
        "    \n",
        "    # Get prediction probabilities\n",
        "    pred_probs = Y_predicted_probs[sample_idx]\n",
        "    \n",
        "    # Basic info about the sample\n",
        "    print(f\"\\nðŸ”¹ Sample #{sample_idx}:\")\n",
        "    print(f\"True Class: {attack_categories[y_true]}\")\n",
        "    print(f\"Predicted Class: {attack_categories[y_pred]}\")\n",
        "    print(\"Prediction Probabilities:\")\n",
        "    for i, category in enumerate(attack_categories):\n",
        "        print(f\"  {category}: {pred_probs[i]:.4f}\")\n",
        "    \n",
        "    # Print classification context\n",
        "    print(\"\\nðŸ”¹ Classification Context:\")\n",
        "    if y_true == y_pred:\n",
        "        print(f\"âœ… This sample was correctly classified as {attack_categories[y_pred]}.\")\n",
        "        print(f\"Description: {attack_chars[attack_categories[y_pred]]}\")\n",
        "    else:\n",
        "        print(f\"âŒ This sample was misclassified as {attack_categories[y_pred]} when it was actually {attack_categories[y_true]}.\")\n",
        "        print(f\"True class ({attack_categories[y_true]}): {attack_chars[attack_categories[y_true]]}\")\n",
        "        print(f\"Predicted class ({attack_categories[y_pred]}): {attack_chars[attack_categories[y_pred]]}\")\n",
        "    \n",
        "    try:\n",
        "        # Create explainer with fewer background samples\n",
        "        print(\"\\nðŸ”¹ Creating SHAP explainer with smaller background...\")\n",
        "        explainer = shap.GradientExplainer(model, background)\n",
        "        \n",
        "        # Generate SHAP values for the sample\n",
        "        print(\"Calculating SHAP values...\")\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "        \n",
        "        # Debug SHAP values structure\n",
        "        print(\"\\nðŸ”¹ Debugging SHAP value structure:\")\n",
        "        print(f\"SHAP values type: {type(shap_values)}\")\n",
        "        if isinstance(shap_values, list):\n",
        "            print(f\"SHAP values list length: {len(shap_values)}\")\n",
        "            for i, sv in enumerate(shap_values):\n",
        "                print(f\"  Class {i} shape: {np.array(sv).shape}\")\n",
        "                \n",
        "            # Standard SHAP list - use normal approach\n",
        "            class_analyses = {}\n",
        "            # Loop through each attack category if explain_all_classes is True\n",
        "            classes_to_analyze = range(len(attack_categories)) if explain_all_classes else [y_pred]\n",
        "            \n",
        "            for class_idx in classes_to_analyze:\n",
        "                is_predicted = (class_idx == y_pred)\n",
        "                is_true = (class_idx == y_true)\n",
        "                class_name = attack_categories[class_idx]\n",
        "                \n",
        "                status = \"\"\n",
        "                if is_predicted and is_true:\n",
        "                    status = \"ðŸŸ¢ PREDICTED & TRUE CLASS\"\n",
        "                elif is_predicted:\n",
        "                    status = \"ðŸŸ¡ PREDICTED CLASS\"\n",
        "                elif is_true:\n",
        "                    status = \"ðŸ”µ TRUE CLASS\"\n",
        "                \n",
        "                print(f\"\\nðŸ”¹ Analyzing SHAP values for class: {class_name} {status}\")\n",
        "                \n",
        "                # Get SHAP values for this class\n",
        "                class_shap = shap_values[class_idx]\n",
        "                flat_shap = class_shap.flatten()\n",
        "                \n",
        "                # Map to features\n",
        "                sorted_features = map_pixels_to_features(flat_shap, image_model, num_features)\n",
        "                \n",
        "                if not sorted_features:\n",
        "                    print(f\"No significant features found for {class_name}\")\n",
        "                    continue\n",
        "                \n",
        "                # Create visualization\n",
        "                plt.figure(figsize=(18, 8))\n",
        "                \n",
        "                # 1. Feature Importance Plot\n",
        "                plt.subplot(1, 2, 1)\n",
        "                features = [x[0] for x in sorted_features]\n",
        "                importance_values = [x[1]['importance'] for x in sorted_features]\n",
        "                colors = ['red' if x[1]['type'] == 'positive' else 'blue' for x in sorted_features]\n",
        "                \n",
        "                # Create horizontal bar plot\n",
        "                bars = plt.barh(range(len(features)), importance_values, color=colors)\n",
        "                \n",
        "                # Add values to bars\n",
        "                if importance_values:\n",
        "                    max_importance = max([abs(x) for x in importance_values])\n",
        "                    for i, bar in enumerate(bars):\n",
        "                        value = importance_values[i]\n",
        "                        if value > 0:\n",
        "                            plt.text(value + max_importance * 0.01, i, f\"{value:.2e}\", va='center')\n",
        "                        else:\n",
        "                            plt.text(value - max_importance * 0.05, i, f\"{value:.2e}\", va='center', ha='right', color='white')\n",
        "                \n",
        "                plt.yticks(range(len(features)), [f\"{f[:25]}...\" if len(f) > 25 else f for f in features])\n",
        "                plt.xlabel('SHAP Value')\n",
        "                plt.title(f'Top {num_features} Feature Contributions for {class_name}')\n",
        "                \n",
        "                # Add legend\n",
        "                red_patch = mpatches.Patch(color='red', label='Increases probability')\n",
        "                blue_patch = mpatches.Patch(color='blue', label='Decreases probability')\n",
        "                plt.legend(handles=[red_patch, blue_patch], loc='lower right')\n",
        "                \n",
        "                # 2. Pixel Location Plot\n",
        "                plt.subplot(1, 2, 2)\n",
        "                pixel_values = np.zeros((image_size1, image_size2))\n",
        "                \n",
        "                # Fill in pixel values from top features\n",
        "                for feature_name, details in sorted_features:\n",
        "                    x, y = details['coordinates']\n",
        "                    # Ensure coordinates are within bounds and adjust to 0-indexed\n",
        "                    x = min(x-1, image_size1-1)\n",
        "                    y = min(y-1, image_size2-1)\n",
        "                    pixel_values[y, x] = details['importance']  # Note: y, x for array indexing\n",
        "                \n",
        "                # Create heatmap\n",
        "                sns.heatmap(pixel_values, cmap='RdBu_r', center=0)\n",
        "                plt.title(f'Feature Location Heatmap for {class_name}')\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                \n",
        "                # Save the figure if output directory is provided\n",
        "                if output_dir:\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    plt.savefig(os.path.join(output_dir, f'feature_importance_{class_name}.png'))\n",
        "                \n",
        "                plt.show()\n",
        "                \n",
        "                # Create HTML table for feature importance with enhanced explanations\n",
        "                table_html = f\"\"\"\n",
        "                <h3>Top {min(num_features, len(sorted_features))} Features for {class_name} Classification {status}</h3>\n",
        "                <table style=\"width:100%; border-collapse: collapse; margin-bottom: 20px;\">\n",
        "                    <tr style=\"background-color: #f2f2f2;\">\n",
        "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Rank</th>\n",
        "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Feature</th>\n",
        "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Description</th>\n",
        "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Importance</th>\n",
        "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Impact</th>\n",
        "                    </tr>\n",
        "                \"\"\"\n",
        "                \n",
        "                for i, (feature_name, details) in enumerate(sorted_features):\n",
        "                    color = \"#ffebee\" if details['type'] == 'positive' else \"#e3f2fd\"\n",
        "                    impact = \"Increases probability\" if details['type'] == 'positive' else \"Decreases probability\"\n",
        "                    description = feature_descriptions.get(feature_name, \"No description available\")\n",
        "                    \n",
        "                    table_html += f\"\"\"\n",
        "                    <tr style=\"background-color: {color}\">\n",
        "                        <td style=\"padding: 12px; border: 1px solid #ddd;\">{i+1}</td>\n",
        "                        <td style=\"padding: 12px; border: 1px solid #ddd;\">{feature_name}</td>\n",
        "                        <td style=\"padding: 12px; border: 1px solid #ddd;\">{description}</td>\n",
        "                        <td style=\"padding: 12px; border: 1px solid #ddd;\">{details['abs_importance']:.2e}</td>\n",
        "                        <td style=\"padding: 12px; border: 1px solid #ddd;\">{impact}</td>\n",
        "                    </tr>\n",
        "                    \"\"\"\n",
        "                \n",
        "                table_html += \"</table>\"\n",
        "                display(HTML(table_html))\n",
        "                \n",
        "                # Provide interpretation of the results\n",
        "                print(\"\\nðŸ”¹ Interpretation:\")\n",
        "                print(f\"For {class_name} classification, the model is:\")\n",
        "                \n",
        "                # Get top 3 positive and negative features if available\n",
        "                positive_features = [(name, details) for name, details in sorted_features if details['type'] == 'positive']\n",
        "                negative_features = [(name, details) for name, details in sorted_features if details['type'] == 'negative']\n",
        "                \n",
        "                # Sort by absolute importance\n",
        "                positive_features = sorted(positive_features, key=lambda x: x[1]['abs_importance'], reverse=True)[:3]\n",
        "                negative_features = sorted(negative_features, key=lambda x: x[1]['abs_importance'], reverse=True)[:3]\n",
        "                \n",
        "                if positive_features:\n",
        "                    print(\"  Primarily focusing on (increasing probability):\")\n",
        "                    for name, details in positive_features:\n",
        "                        print(f\"    - {name}: {feature_descriptions.get(name, 'No description available')}\")\n",
        "                \n",
        "                if negative_features:\n",
        "                    print(\"  Being discouraged by (decreasing probability):\")\n",
        "                    for name, details in negative_features:\n",
        "                        print(f\"    - {name}: {feature_descriptions.get(name, 'No description available')}\")\n",
        "                \n",
        "                print(f\"\\nThis {'' if is_predicted else 'would have '} led to a {class_name} classification.\")\n",
        "                \n",
        "                # Add to class_analyses\n",
        "                class_analyses[class_name] = {\n",
        "                    'shap_values': class_shap, \n",
        "                    'feature_importance': sorted_features,\n",
        "                    'is_predicted': is_predicted,\n",
        "                    'is_true': is_true\n",
        "                }\n",
        "            \n",
        "        elif isinstance(shap_values, np.ndarray):\n",
        "            print(f\"SHAP values shape: {shap_values.shape}\")\n",
        "            \n",
        "            # Check if we have multi-class values in the last dimension\n",
        "            if shap_values.shape[-1] == len(attack_categories):\n",
        "                print(\"âœ… Multi-class SHAP values detected in the last dimension\")\n",
        "                # Use direct analysis for multi-dimensional array\n",
        "                class_analyses = analyze_direct_shap(shap_values, X_sample, image_model, \n",
        "                                                   attack_categories, feature_descriptions, \n",
        "                                                   attack_chars, y_pred, y_true, \n",
        "                                                   num_features, output_dir)\n",
        "            else:\n",
        "                print(\"âš ï¸ Unexpected SHAP values shape - cannot process\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\"âš ï¸ Unexpected SHAP values type - cannot process\")\n",
        "            return None\n",
        "        \n",
        "        # Create cross-class comparison table if we have multiple classes\n",
        "        if len(class_analyses) > 1:\n",
        "            print(\"\\nðŸ”¹ Cross-Class Feature Comparison:\")\n",
        "            all_important_features = set()\n",
        "            class_feature_importance = {}\n",
        "            \n",
        "            for class_name, analysis in class_analyses.items():\n",
        "                if 'feature_importance' in analysis:  # Check if analysis has feature importance\n",
        "                    class_feature_importance[class_name] = {}\n",
        "                    for feature_name, details in analysis['feature_importance']:\n",
        "                        all_important_features.add(feature_name)\n",
        "                        class_feature_importance[class_name][feature_name] = details['importance']\n",
        "            \n",
        "            # Create comparison table\n",
        "            comparison_html = \"\"\"\n",
        "            <h3>Cross-Class Feature Comparison</h3>\n",
        "            <p>This table shows how each important feature affects the probability of each attack class.</p>\n",
        "            <table style=\"width:100%; border-collapse: collapse; margin-bottom: 20px;\">\n",
        "                <tr style=\"background-color: #f2f2f2;\">\n",
        "                    <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Feature</th>\n",
        "            \"\"\"\n",
        "            \n",
        "            # Add class headers\n",
        "            for class_name in class_feature_importance.keys():\n",
        "                comparison_html += f'<th style=\"padding: 12px; text-align: center; border: 1px solid #ddd;\">{class_name}</th>'\n",
        "            \n",
        "            comparison_html += \"</tr>\"\n",
        "            \n",
        "            # Add feature rows\n",
        "            for feature in sorted(all_important_features):\n",
        "                comparison_html += f\"\"\"\n",
        "                <tr>\n",
        "                    <td style=\"padding: 12px; border: 1px solid #ddd;\">{feature}</td>\n",
        "                \"\"\"\n",
        "                \n",
        "                for class_name in class_feature_importance.keys():\n",
        "                    importance = class_feature_importance[class_name].get(feature, 0)\n",
        "                    if abs(importance) < 1e-10:\n",
        "                        # Negligible impact\n",
        "                        cell_style = 'background-color: #f9f9f9; color: #888;'\n",
        "                        cell_text = \"0\"\n",
        "                    elif importance > 0:\n",
        "                        # Positive impact (increases probability)\n",
        "                        intensity = min(255, int(255 * (1 - min(1, importance * 100))))\n",
        "                        cell_style = f'background-color: rgba(255, {intensity}, {intensity}, 0.8);'\n",
        "                        cell_text = f\"+{importance:.2e}\"\n",
        "                    else:\n",
        "                        # Negative impact (decreases probability)\n",
        "                        intensity = min(255, int(255 * (1 - min(1, abs(importance) * 100))))\n",
        "                        cell_style = f'background-color: rgba({intensity}, {intensity}, 255, 0.8);'\n",
        "                        cell_text = f\"{importance:.2e}\"\n",
        "                    \n",
        "                    comparison_html += f'<td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; {cell_style}\">{cell_text}</td>'\n",
        "                \n",
        "                comparison_html += \"</tr>\"\n",
        "            \n",
        "            comparison_html += \"</table>\"\n",
        "            display(HTML(comparison_html))\n",
        "            \n",
        "            # Find distinguishing features if there's a misclassification\n",
        "            if y_true != y_pred and attack_categories[y_true] in class_feature_importance and attack_categories[y_pred] in class_feature_importance:\n",
        "                print(\"\\nðŸ”¹ Cross-Class Insights:\")\n",
        "                \n",
        "                true_class = attack_categories[y_true]\n",
        "                pred_class = attack_categories[y_pred]\n",
        "                \n",
        "                true_class_features = class_feature_importance.get(true_class, {})\n",
        "                pred_class_features = class_feature_importance.get(pred_class, {})\n",
        "                \n",
        "                # Find features that strongly favor the true class but were missed\n",
        "                distinguishing_features = []\n",
        "                for feature, true_importance in true_class_features.items():\n",
        "                    pred_importance = pred_class_features.get(feature, 0)\n",
        "                    if true_importance > 0 and (true_importance > pred_importance):\n",
        "                        distinguishing_features.append((feature, true_importance - pred_importance))\n",
        "                \n",
        "                # Sort by importance difference\n",
        "                distinguishing_features.sort(key=lambda x: x[1], reverse=True)\n",
        "                \n",
        "                if distinguishing_features:\n",
        "                    print(f\"Features that should have indicated {true_class} but were overlooked:\")\n",
        "                    for feature, diff in distinguishing_features[:3]:  # Top 3\n",
        "                        print(f\"  - {feature}: {feature_descriptions.get(feature, 'No description available')}\")\n",
        "                else:\n",
        "                    print(f\"No clear distinguishing features found that would indicate {true_class} over {pred_class}.\")\n",
        "        \n",
        "        # Try to show original SHAP image plot\n",
        "        try:\n",
        "            print(\"\\nðŸ”¹ Original SHAP Visualization (Pixel Values):\")\n",
        "            shap.image_plot(shap_values, X_sample, show=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate SHAP image plot: {e}\")\n",
        "        \n",
        "        # Return results\n",
        "        return {\n",
        "            'sample_idx': sample_idx,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred,\n",
        "            'pred_probs': pred_probs,\n",
        "            'class_analyses': class_analyses\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸ Error generating SHAP explanations: {e}\")\n",
        "        print(\"\\nFalling back to simplified feature importance analysis...\")\n",
        "        \n",
        "        # Create a simplified feature importance visualization\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.title(f\"Sample #{sample_idx}: {attack_categories[y_true]} classified as {attack_categories[y_pred]}\")\n",
        "        plt.imshow(X_sample[0, :, :, 0], cmap='viridis')\n",
        "        plt.colorbar(label='Feature Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Return basic information\n",
        "        return {\n",
        "            'sample_idx': sample_idx,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred,\n",
        "            'pred_probs': pred_probs,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Step 4: Configure parameters and execute analysis\n",
        "# Update these paths to match your environment\n",
        "param = {\n",
        "    \"Max_A_Size\": 10,\n",
        "    \"Max_B_Size\": 10,\n",
        "    \"Dynamic_Size\": False,\n",
        "    'Metod': 'tSNE',\n",
        "    \"ValidRatio\": 0.1,\n",
        "    \"seed\": 180,\n",
        "    \"dir\": \"D:/IDS/Code/Datasets/NSLKDD/\",  # Update with your path\n",
        "    \"res\": \"D:/IDS/Code/Results/\",  # Update with your path\n",
        "    \"Mode\": \"CNN2\",  # CNN_Nature or CNN2\n",
        "    \"LoadFromJson\": True,\n",
        "    \"mutual_info\": True,\n",
        "}\n",
        "\n",
        "# Set paths\n",
        "model_path = param[\"dir\"] + \"best_model.keras\"\n",
        "test_data_path = param[\"dir\"] + f'test_{param[\"Max_A_Size\"]}x{param[\"Max_B_Size\"]}_{\"MI\" if param[\"mutual_info\"] else \"Mean\"}.pickle'\n",
        "test_labels_path = param[\"dir\"] + \"Ytest.pickle\"\n",
        "image_model_path = param[\"dir\"] + f'image_model_{param[\"Max_A_Size\"]}x{param[\"Max_B_Size\"]}_{\"MI\" if param[\"mutual_info\"] else \"Mean\"}.pickle'\n",
        "output_dir = param[\"res\"] + \"explanations/\"\n",
        "\n",
        "# Update image model with feature names\n",
        "update_image_model_with_features(image_model_path, param)\n",
        "\n",
        "# Generate explanations\n",
        "results = explain_multi_class_prediction(\n",
        "    model_path=model_path,\n",
        "    test_data_path=test_data_path,\n",
        "    test_labels_path=test_labels_path,\n",
        "    image_model_path=image_model_path,\n",
        "    num_features=15,\n",
        "    output_dir=output_dir,\n",
        "    explain_all_classes=True  # Analyze all classes, not just predicted/true\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m2by81Ycp1Ke",
        "d3-ITpNnp9vR",
        "iCBvyo-1uWzy",
        "nlrDyDGkuerB",
        "ddltk-WykX1z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
