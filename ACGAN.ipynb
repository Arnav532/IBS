{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4484,
     "status": "ok",
     "timestamp": 1736237088477,
     "user": {
      "displayName": "Vidhya Vijayakumar",
      "userId": "10010028668194528344"
     },
     "user_tz": -330
    },
    "id": "HYnLqMlqDRc2",
    "outputId": "960d7328-b655-4d6f-e01b-33c0c23acef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:An interactive session is already active. This can cause out-of-memory errors or some other unexpected errors (due to the unpredictable timing of garbage collection) in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s). Please use `tf.Session()` if you intend to productionize.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# Close any existing sessions\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Configure new session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xppE8acPcTXJ"
   },
   "outputs": [],
   "source": [
    "'''GAN model builder and util functions\n",
    "\n",
    "[1] Radford, Alec, Luke Metz, and Soumith Chintala.\n",
    "\"Unsupervised representation learning with deep convolutional\n",
    "generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015).\n",
    "\n",
    "'''\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def generator(inputs,\n",
    "              image_size,\n",
    "              activation='sigmoid',\n",
    "              labels=None,\n",
    "              codes=None):\n",
    "    \"\"\"Build a Generator Model\n",
    "\n",
    "    Stack of BN-ReLU-Conv2DTranpose to generate fake images.\n",
    "    Output activation is sigmoid instead of tanh in [1].\n",
    "    Sigmoid converges easily.\n",
    "\n",
    "    Arguments:\n",
    "        inputs (Layer): Input layer of the generator (the z-vector)\n",
    "        image_size (int): Target size of one side\n",
    "            (assuming square image)\n",
    "        activation (string): Name of output activation layer\n",
    "        labels (tensor): Input labels\n",
    "        codes (list): 2-dim disentangled codes for InfoGAN\n",
    "\n",
    "    Returns:\n",
    "        Model: Generator Model\n",
    "    \"\"\"\n",
    "\n",
    "    inputs_tensor = []\n",
    "    image_resize = image_size//2\n",
    "    # network parameters\n",
    "    kernel_size = 5\n",
    "    layer_filters = [256,128, 64, 32, 1]\n",
    "\n",
    "    if labels is not None:\n",
    "        if codes is None:\n",
    "            # ACGAN labels\n",
    "            # concatenate z noise vector and one-hot labels\n",
    "            inputs_tensor = [inputs, labels]\n",
    "        else:\n",
    "            # infoGAN codes\n",
    "            # concatenate z noise vector,\n",
    "            # one-hot labels and codes 1 & 2\n",
    "            inputs_tensor = [inputs, labels] + codes\n",
    "        x = concatenate(inputs_tensor, axis=1)\n",
    "    elif codes is not None:\n",
    "        # generator 0 of StackedGAN\n",
    "        inputs_tensor = [inputs, codes]\n",
    "        x = concatenate(inputs_tensor, axis=1)\n",
    "    else:\n",
    "        # default input is just 100-dim noise (z-code)\n",
    "        x = inputs\n",
    "\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(x)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # first two convolution layers use strides = 2\n",
    "        # the last two use strides = 1\n",
    "        if filters > layer_filters[-4]:\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='same')(x)\n",
    "    import tensorflow.python.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "    if activation is not None:\n",
    "        x = Activation(activation)(x)\n",
    "\n",
    "\n",
    "    # generator output is the synthesized image x\n",
    "    return Model(inputs_tensor if len(inputs_tensor) > 1 else inputs, x, name='generator')\n",
    "\n",
    "# class MyLeakyReLU(Layer):\n",
    "#     def __init__(self, alpha=0.2, **kwargs):\n",
    "#         super(MyLeakyReLU, self).__init__(**kwargs)\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         return tf.keras.backend.relu(inputs, alpha=self.alpha)\n",
    "\n",
    "\n",
    "def discriminator(inputs,\n",
    "                  activation='sigmoid',\n",
    "                  num_labels=None,\n",
    "                  num_codes=None):\n",
    "    \"\"\"Build a Discriminator Model\n",
    "\n",
    "    Stack of LeakyReLU-Conv2D to discriminate real from fake\n",
    "    The network does not converge with BN so it is not used here\n",
    "    unlike in [1]\n",
    "\n",
    "    Arguments:\n",
    "        inputs (Layer): Input layer of the discriminator (the image)\n",
    "        activation (string): Name of output activation layer\n",
    "        num_labels (int): Dimension of one-hot labels for ACGAN & InfoGAN\n",
    "        num_codes (int): num_codes-dim Q network as output\n",
    "                    if StackedGAN or 2 Q networks if InfoGAN\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Model: Discriminator Model\n",
    "    \"\"\"\n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    x = inputs\n",
    "    for filters in layer_filters:\n",
    "        # first 3 convolution layers use strides = 2\n",
    "        # last one uses strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    # default output is probability that the image is real\n",
    "    outputs = Dense(1)(x)\n",
    "    if activation is not None:\n",
    "        print(activation)\n",
    "        outputs = Activation(activation)(outputs)\n",
    "\n",
    "    if num_labels:\n",
    "        # ACGAN and InfoGAN have 2nd output\n",
    "        # 2nd output is 10-dim one-hot vector of label\n",
    "        layer = Dense(layer_filters[-2])(x)\n",
    "        labels = Dense(num_labels)(layer)\n",
    "        labels = Activation('softmax', name='label')(labels)\n",
    "        if num_codes is None:\n",
    "            outputs = [outputs, labels]\n",
    "        else:\n",
    "            # InfoGAN have 3rd and 4th outputs\n",
    "            # 3rd output is 1-dim continous Q of 1st c given x\n",
    "            code1 = Dense(1)(layer)\n",
    "            code1 = Activation('sigmoid', name='code1')(code1)\n",
    "\n",
    "            # 4th output is 1-dim continuous Q of 2nd c given x\n",
    "            code2 = Dense(1)(layer)\n",
    "            code2 = Activation('sigmoid', name='code2')(code2)\n",
    "\n",
    "            outputs = [outputs, labels, code1, code2]\n",
    "    elif num_codes is not None:\n",
    "        # StackedGAN Q0 output\n",
    "        # z0_recon is reconstruction of z0 normal distribution\n",
    "        z0_recon = Dense(num_codes)(x)\n",
    "        z0_recon = Activation('tanh', name='z0')(z0_recon)\n",
    "        outputs = [outputs, z0_recon]\n",
    "\n",
    "    return Model(inputs, outputs, name='discriminator')\n",
    "\n",
    "\n",
    "def train(models, x_train, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "\n",
    "    Alternately train Discriminator and Adversarial networks by batch.\n",
    "    Discriminator is trained first with properly real and fake images.\n",
    "    Adversarial is trained next with fake images pretending to be real\n",
    "    Generate sample images per save_interval.\n",
    "\n",
    "    # Arguments\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        x_train (tensor): Train images\n",
    "        params (list) : Networks parameters\n",
    "\n",
    "    \"\"\"\n",
    "    # the GAN models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, model_name = params\n",
    "    # the generator image is saved every 500 steps\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output\n",
    "    # evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "    for i in range(train_steps):\n",
    "        # train the discriminator for 1 batch\n",
    "        # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "        # randomly pick real images from dataset\n",
    "        rand_indexes = np.random.randint(0,\n",
    "                                         train_size,\n",
    "                                         size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # generate fake images from noise using generator\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0,\n",
    "                                  size=[batch_size, latent_size])\n",
    "        # generate fake images\n",
    "        fake_images = generator.predict(noise)\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # train discriminator network, log the loss and accuracy\n",
    "        loss, acc = discriminator.train_on_batch(x, y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        # train the adversarial network for 1 batch\n",
    "        # 1 batch of fake images with label=1.0\n",
    "        # since the discriminator weights are frozen\n",
    "        # in adversarial network only the generator is trained\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0,\n",
    "                                  size=[batch_size, latent_size])\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # train the adversarial network\n",
    "        # note that unlike in discriminator training,\n",
    "        # we do not save the fake images in a variable\n",
    "        # the fake images go to the discriminator input\n",
    "        # of the adversarial for classification\n",
    "        # log the loss and accuracy\n",
    "        loss, acc = adversarial.train_on_batch(noise, y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        show=False,\n",
    "                        step=(i + 1),\n",
    "                        model_name=model_name)\n",
    "\n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded\n",
    "    # for future MNIST digit generation\n",
    "    generator.save(model_name + \".h5\")\n",
    "\n",
    "\n",
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                noise_label=None,\n",
    "                noise_codes=None,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    \"\"\"Generate fake images and plot them\n",
    "\n",
    "    For visualization purposes, generate fake images\n",
    "    then plot them in a square grid\n",
    "\n",
    "    # Arguments\n",
    "        generator (Model): The Generator Model for\n",
    "            fake images generation\n",
    "        noise_input (ndarray): Array of z-vectors\n",
    "        show (bool): Whether to show plot or not\n",
    "        step (int): Appended to filename of the save images\n",
    "        model_name (string): Model name\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    print(filename)\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    if noise_label is not None:\n",
    "        noise_input = [noise_input, noise_label]\n",
    "        if noise_codes is not None:\n",
    "            noise_input += noise_codes\n",
    "\n",
    "    images = generator.predict(noise_input)\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "def test_generator(generator):\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "    plot_images(generator,\n",
    "                noise_input=noise_input,\n",
    "                show=True,\n",
    "                model_name=\"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: D:/IDS/Datasets/AAGM\\train_10x10_MI.pickle\n",
      "Loading labels from: D:/IDS/Datasets/AAGM\\YTrain.pickle\n",
      "Input image size: 10x10\n",
      "Number of classes: 2\n",
      "\n",
      "Generator Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ z_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ label_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ z_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ label_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">237,312</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,216</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_4  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_10      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_5  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │ leaky_re_lu_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> │ leaky_re_lu_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ z_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ label_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ z_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ label_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)      │    \u001b[38;5;34m237,312\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)      │      \u001b[38;5;34m9,216\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_4  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m295,040\u001b[0m │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_10      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_5  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │     \u001b[38;5;34m73,792\u001b[0m │ leaky_re_lu_10[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1\u001b[0m) │        \u001b[38;5;34m577\u001b[0m │ leaky_re_lu_11[\u001b[38;5;34m0\u001b[0m… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">616,705</span> (2.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m616,705\u001b[0m (2.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">611,713</span> (2.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m611,713\u001b[0m (2.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,992</span> (19.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,992\u001b[0m (19.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminator Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"discriminator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"discriminator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ discriminator_input │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ discriminator_in… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_12      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_13      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_14      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ source (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ label (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ discriminator_input │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │        \u001b[38;5;34m640\u001b[0m │ discriminator_in… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_12      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_12[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m73,856\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_13      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m819,456\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_14      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_14[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ source (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ label (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m514\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">894,723</span> (3.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m894,723\u001b[0m (3.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">894,723</span> (3.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m894,723\u001b[0m (3.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IDS\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step\n",
      "Error during training: For a model with multiple outputs, when providing the `metrics` argument as a list, it should have as many entries as the model has outputs. Received:\n",
      "metrics=['accuracy']\n",
      "of length 1 whereas the model has 2 outputs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IDS\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def build_generator(latent_size, image_size, num_labels):\n",
    "    \"\"\"\n",
    "    Build generator model with correct output dimensions.\n",
    "    For 10x10 output:\n",
    "    - Start with 3x3\n",
    "    - First upsample to 5x5\n",
    "    - Second upsample to 10x10\n",
    "    \"\"\"\n",
    "    # Image generator input\n",
    "    inputs = Input(shape=(latent_size,), name='z_input')\n",
    "    label_input = Input(shape=(num_labels,), name='label_input')\n",
    "    \n",
    "    # Concatenate noise and label inputs\n",
    "    x = tf.keras.layers.concatenate([inputs, label_input])\n",
    "    \n",
    "    # First dense layer\n",
    "    # For 10x10 target, start with 3x3\n",
    "    init_size = 3\n",
    "    filters = 256\n",
    "    \n",
    "    x = Dense(filters * init_size * init_size)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Reshape((init_size, init_size, filters))(x)\n",
    "\n",
    "    # First upsampling: 3x3 -> 5x5\n",
    "    x = Conv2DTranspose(128, 3, strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "\n",
    "    # Second upsampling: 5x5 -> 10x10\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "\n",
    "    # Final convolution\n",
    "    x = Conv2D(1, 3, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    # Verify output shape\n",
    "    assert x.shape[1:3] == (image_size, image_size), f\"Generator output shape mismatch: expected {(image_size, image_size)}, got {x.shape[1:3]}\"\n",
    "    \n",
    "    generator = Model([inputs, label_input], x, name='generator')\n",
    "    return generator\n",
    "\n",
    "def build_discriminator(input_shape, num_labels):\n",
    "    \"\"\"\n",
    "    Build discriminator model suitable for 10x10 input images.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "    \n",
    "    x = Conv2D(64, 3, strides=1, padding='same')(inputs)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, 3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Source output (real/fake)\n",
    "    source_output = Dense(1, activation='sigmoid', name='source')(x)\n",
    "    \n",
    "    # Label output\n",
    "    label_output = Dense(num_labels, activation='softmax', name='label')(x)\n",
    "    \n",
    "    discriminator = Model(inputs, [source_output, label_output], name='discriminator')\n",
    "    return discriminator\n",
    "\n",
    "def load_data(params):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    try:\n",
    "        base_dir = params[\"dir\"]\n",
    "        method = 'MI' if params['mutual_info'] else 'Mean'\n",
    "        \n",
    "        data_filename = os.path.join(\n",
    "            base_dir, \n",
    "            f'train_{params[\"Max_A_Size\"]}x{params[\"Max_B_Size\"]}_{method}.pickle'\n",
    "        )\n",
    "        label_filename = os.path.join(base_dir, 'YTrain.pickle')\n",
    "        \n",
    "        print(f\"Loading data from: {data_filename}\")\n",
    "        print(f\"Loading labels from: {label_filename}\")\n",
    "        \n",
    "        with open(data_filename, 'rb') as f:\n",
    "            x_train = pickle.load(f)\n",
    "        with open(label_filename, 'rb') as f:\n",
    "            y_train = pickle.load(f)\n",
    "            \n",
    "        return np.asarray(x_train), np.asarray(y_train)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find data files in {base_dir}\")\n",
    "        print(f\"Expected files: \\n- {data_filename}\\n- {label_filename}\")\n",
    "        raise e\n",
    "\n",
    "def train(models, data, params):\n",
    "    \"\"\"Train the ACGAN model.\"\"\"\n",
    "    generator, discriminator, adversarial = models\n",
    "    x_train, y_train = data\n",
    "    batch_size, latent_size, train_steps, num_labels = params\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(train_steps):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        real_images = x_train[idx]\n",
    "        real_labels = y_train[idx]\n",
    "        \n",
    "        # Generate fake images\n",
    "        noise = np.random.uniform(-1.0, 1.0, (batch_size, latent_size))\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size)]\n",
    "        fake_images = generator.predict([noise, fake_labels])\n",
    "        \n",
    "        # Prepare discriminator training data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        labels = np.concatenate((real_labels, fake_labels))\n",
    "        y = np.ones((2 * batch_size, 1))\n",
    "        y[batch_size:, :] = 0\n",
    "        \n",
    "        # Train discriminator\n",
    "        d_loss = discriminator.train_on_batch(x, [y, labels])\n",
    "        \n",
    "        # Train generator through adversarial model\n",
    "        noise = np.random.uniform(-1.0, 1.0, (batch_size, latent_size))\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size)]\n",
    "        y = np.ones((batch_size, 1))\n",
    "        \n",
    "        a_loss = adversarial.train_on_batch([noise, fake_labels], [y, fake_labels])\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'Step {step}: [D loss: {d_loss[0]:.4f}, acc: {d_loss[3]:.4f}] [A loss: {a_loss[0]:.4f}]')\n",
    "\n",
    "def build_and_train_models(params):\n",
    "    \"\"\"Main function to build and train the ACGAN.\"\"\"\n",
    "    session = configure_session()\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        x_train, y_train = load_data(params)\n",
    "        image_size = x_train.shape[1]\n",
    "        print(f\"Input image size: {image_size}x{image_size}\")\n",
    "        \n",
    "        # Reshape and normalize data\n",
    "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "        x_train = x_train.astype('float32') / 255 * 2 - 1  # Scale to [-1, 1]\n",
    "        \n",
    "        # Convert labels to categorical\n",
    "        num_labels = len(np.unique(y_train))\n",
    "        y_train = to_categorical(y_train)\n",
    "        print(f\"Number of classes: {num_labels}\")\n",
    "        \n",
    "        # Network parameters\n",
    "        latent_size = 100\n",
    "        batch_size = 32  # Reduced batch size for smaller images\n",
    "        train_steps = 40000\n",
    "        lr = 1e-4  # Reduced learning rate for stability\n",
    "        decay = 6e-8\n",
    "        \n",
    "        # Build models\n",
    "        generator = build_generator(latent_size, image_size, num_labels)\n",
    "        discriminator = build_discriminator((image_size, image_size, 1), num_labels)\n",
    "        \n",
    "        # Print model summaries\n",
    "        print(\"\\nGenerator Summary:\")\n",
    "        generator.summary()\n",
    "        print(\"\\nDiscriminator Summary:\")\n",
    "        discriminator.summary()\n",
    "        \n",
    "        # Build adversarial model\n",
    "        discriminator.trainable = False\n",
    "        noise_input = Input(shape=(latent_size,))\n",
    "        label_input = Input(shape=(num_labels,))\n",
    "        gen_output = generator([noise_input, label_input])\n",
    "        adversarial_output = discriminator(gen_output)\n",
    "        adversarial = Model([noise_input, label_input], adversarial_output)\n",
    "        \n",
    "        # Compile models\n",
    "        optimizer = RMSprop(learning_rate=lr, decay=decay)\n",
    "        loss = ['binary_crossentropy', 'categorical_crossentropy']\n",
    "        \n",
    "        discriminator.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        adversarial.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        \n",
    "        # Train models\n",
    "        models = (generator, discriminator, adversarial)\n",
    "        data = (x_train, y_train)\n",
    "        params = (batch_size, latent_size, train_steps, num_labels)\n",
    "        \n",
    "        train(models, data, params)\n",
    "        \n",
    "        return generator\n",
    "        \n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = {\n",
    "        \"Max_A_Size\": 10,\n",
    "        \"Max_B_Size\": 10,\n",
    "        \"Dynamic_Size\": False,\n",
    "        \"Metod\": \"tSNE\",\n",
    "        \"ValidRatio\": 0.1,\n",
    "        \"seed\": 180,\n",
    "        \"dir\": \"D:/IDS/Datasets/AAGM\",\n",
    "        \"Mode\": \"CNN2\",\n",
    "        \"LoadFromJson\": False,\n",
    "        \"mutual_info\": True,\n",
    "        \"hyper_opt_evals\": 20,\n",
    "        \"epoch\": 150,\n",
    "        \"No_0_MI\": False,\n",
    "        \"autoencoder\": False,\n",
    "        \"cut\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        generator = build_and_train_models(params)\n",
    "        generator.save('acgan_generator.h5')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
